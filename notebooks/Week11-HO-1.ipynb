{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a3da80-19b6-429d-94b2-fe3f149054aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote probe snapshot -> /home/manny-buff/projects/capstone/week11-hw/configs/week11_sys_probe.json\n",
      "\n",
      "Week 11 Probe Summary\n",
      "------------------------------------------------------------------------\n",
      "Root: /home/manny-buff/projects/capstone/week11-hw\n",
      "Models (RO): /home/manny-buff/projects/capstone/hw-rag/models\n",
      "Data   (RO): /home/manny-buff/projects/capstone/hw-rag/data\n",
      "------------------------------------------------------------------------\n",
      "Dirs OK: 14/14\n",
      "All expected dirs present.\n",
      "------------------------------------------------------------------------\n",
      "Model candidates (by hint):\n",
      "  Qwen2.5-VL-2B-Instruct: 0 files\n",
      "  Intern3_5-VL-4B-Instruct: 0 files\n",
      "------------------------------------------------------------------------\n",
      "Key packages:\n",
      "  torch          -> 2.9.0+cu128\n",
      "  torchvision    -> 0.24.0+cu128\n",
      "  torchaudio     -> not_importable (OSError: Could not load this library: /home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/torchaudio/lib/libtorchaudio.so)\n",
      "  transformers   -> 4.45.2\n",
      "  accelerate     -> 1.10.1\n",
      "  bitsandbytes   -> not_importable (ModuleNotFoundError: No module named 'bitsandbytes')\n",
      "  cv2            -> 4.12.0\n",
      "  imageio        -> 2.37.0\n",
      "  imageio_ffmpeg -> not_importable (ModuleNotFoundError: No module named 'imageio_ffmpeg')\n",
      "  PIL            -> 10.4.0\n",
      "  pandas         -> 2.2.3\n",
      "  numpy          -> 2.2.1\n",
      "  networkx       -> 3.3\n",
      "  faiss          -> 1.10.0\n",
      "  faiss_cpu      -> not_importable (ModuleNotFoundError: No module named 'faiss_cpu')\n",
      "  chromadb       -> 1.1.0\n",
      "------------------------------------------------------------------------\n",
      "CUDA/GPU:\n",
      "  available     : True\n",
      "  device_count  : 1\n",
      "  device_name_0 : NVIDIA GeForce RTX 4080\n",
      "  capability_0  : 8.9\n",
      "  nvidia_smi    : /usr/bin/nvidia-smi\n",
      "------------------------------------------------------------------------\n",
      "ffmpeg:\n",
      "  path          : /usr/bin/ffmpeg\n",
      "  version_line  : ffmpeg version 6.1.1-3ubuntu5+esm6 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "------------------------------------------------------------------------\n",
      "Disk (GB):\n",
      "  week11_hw     : {'total_gb': 1261.0, 'used_gb': 417.64, 'free_gb': 786.93}\n",
      "  data_ro       : {'total_gb': 1261.0, 'used_gb': 417.64, 'free_gb': 786.93}\n",
      "  models_ro     : {'total_gb': 1261.0, 'used_gb': 417.64, 'free_gb': 786.93}\n",
      "------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dirs_ok': True,\n",
       " 'have_qwen': False,\n",
       " 'have_intern': False,\n",
       " 'cuda': True,\n",
       " 'ffmpeg': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 · Cell 0 — System & Environment Probe (Week 11: Home Repair, Video Gen + Graph RAG)\n",
    "# - Verifies expected folders and local model presence\n",
    "# - Summarizes Python & key package versions (no installs)\n",
    "# - Checks CUDA/GPU, ffmpeg, and disk space\n",
    "# - Saves a JSON snapshot to configs/week11_sys_probe.json\n",
    "\n",
    "import sys, os, json, shutil, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Paths (edit here if your layout differs) ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "MODELS_RO = Path(\"/home/manny-buff/projects/capstone/hw-rag/models\")\n",
    "DATA_RO   = Path(\"/home/manny-buff/projects/capstone/hw-rag/data\")\n",
    "\n",
    "EXPECTED_DIRS = [\n",
    "    W11,\n",
    "    W11 / \"notebooks\",\n",
    "    W11 / \"src\",\n",
    "    W11 / \"data\",\n",
    "    W11 / \"data\" / \"external\",\n",
    "    W11 / \"artifacts\",\n",
    "    W11 / \"visual_outputs\",\n",
    "    W11 / \"audio_inputs\",\n",
    "    W11 / \"audio_outputs\",\n",
    "    W11 / \"configs\",\n",
    "    W11 / \"reports\",\n",
    "    W11 / \"source_mats\",  # you mentioned docs live here\n",
    "    MODELS_RO,\n",
    "    DATA_RO,\n",
    "]\n",
    "\n",
    "MODEL_HINTS = [\n",
    "    \"Qwen2.5-VL-2B-Instruct\",\n",
    "    \"Intern3_5-VL-4B-Instruct\",\n",
    "]\n",
    "\n",
    "SNAPSHOT = W11 / \"configs\" / \"week11_sys_probe.json\"\n",
    "SNAPSHOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def which(cmd):\n",
    "    try:\n",
    "        return shutil.which(cmd)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def run_cmd(args):\n",
    "    try:\n",
    "        return subprocess.check_output(args, stderr=subprocess.STDOUT).decode(\"utf-8\", \"ignore\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def try_import_version(mod_name):\n",
    "    try:\n",
    "        m = __import__(mod_name)\n",
    "        return getattr(m, \"__version__\", \"installed\")\n",
    "    except Exception as e:\n",
    "        return f\"not_importable ({e.__class__.__name__}: {e})\"\n",
    "\n",
    "def list_model_candidates(root: Path, hints):\n",
    "    results = {}\n",
    "    if not root.exists():\n",
    "        return results\n",
    "    for h in hints:\n",
    "        hits = []\n",
    "        for p in root.rglob(\"*\"):\n",
    "            name = p.name.lower()\n",
    "            if h.lower() in name and p.is_file():\n",
    "                hits.append(str(p))\n",
    "        results[h] = sorted(hits)\n",
    "    return results\n",
    "\n",
    "def disk_usage(path: Path):\n",
    "    try:\n",
    "        usage = shutil.disk_usage(str(path))\n",
    "        return {\"total_gb\": round(usage.total/1e9, 2),\n",
    "                \"used_gb\": round(usage.used/1e9, 2),\n",
    "                \"free_gb\": round(usage.free/1e9, 2)}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# ---------- Folder checks ----------\n",
    "dirs_status = {str(p): (p.exists(), \"dir\" if p.is_dir() else (\"file\" if p.is_file() else \"missing\")) for p in EXPECTED_DIRS}\n",
    "\n",
    "# ---------- Model hints ----------\n",
    "model_files = list_model_candidates(MODELS_RO, MODEL_HINTS)\n",
    "\n",
    "# ---------- Environment checks ----------\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"paths\": {\n",
    "        \"W11\": str(W11),\n",
    "        \"MODELS_RO\": str(MODELS_RO),\n",
    "        \"DATA_RO\": str(DATA_RO),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Core packages\n",
    "pkg_list = [\n",
    "    \"torch\", \"torchvision\", \"torchaudio\",\n",
    "    \"transformers\", \"accelerate\", \"bitsandbytes\",\n",
    "    \"opencv\", \"cv2\", \"imageio\", \"imageio_ffmpeg\", \"PIL\",  # PIL = Pillow\n",
    "    \"tqdm\", \"pandas\", \"numpy\",\n",
    "    # graph / vector db scaffolds\n",
    "    \"networkx\", \"faiss\", \"faiss_cpu\", \"chromadb\",\n",
    "]\n",
    "\n",
    "versions = {}\n",
    "for name in pkg_list:\n",
    "    # normalize import names (opencv-python -> cv2; Pillow -> PIL)\n",
    "    alias = name\n",
    "    if name == \"opencv\": alias = \"cv2\"\n",
    "    if name == \"PIL\": alias = \"PIL\"\n",
    "    try:\n",
    "        mod = __import__(alias)\n",
    "        versions[name] = getattr(mod, \"__version__\", \"installed\")\n",
    "    except Exception as e:\n",
    "        versions[name] = f\"not_importable ({e.__class__.__name__}: {e})\"\n",
    "\n",
    "# CUDA / GPU\n",
    "cuda = {\n",
    "    \"available\": False,\n",
    "    \"device_count\": 0,\n",
    "    \"device_name_0\": None,\n",
    "    \"capability_0\": None,\n",
    "    \"nvidia_smi\": which(\"nvidia-smi\"),\n",
    "}\n",
    "try:\n",
    "    import torch\n",
    "    cuda[\"available\"] = bool(torch.cuda.is_available())\n",
    "    cuda[\"device_count\"] = torch.cuda.device_count()\n",
    "    if cuda[\"available\"] and cuda[\"device_count\"] > 0:\n",
    "        cuda[\"device_name_0\"] = torch.cuda.get_device_name(0)\n",
    "        try:\n",
    "            cap = torch.cuda.get_device_capability(0)\n",
    "            cuda[\"capability_0\"] = f\"{cap[0]}.{cap[1]}\"\n",
    "        except Exception:\n",
    "            cuda[\"capability_0\"] = \"unknown\"\n",
    "except Exception as e:\n",
    "    cuda[\"torch_error\"] = f\"{e.__class__.__name__}: {e}\"\n",
    "\n",
    "# ffmpeg check\n",
    "ffmpeg = {\n",
    "    \"path\": which(\"ffmpeg\"),\n",
    "    \"version_line\": None\n",
    "}\n",
    "if ffmpeg[\"path\"]:\n",
    "    v = run_cmd([\"ffmpeg\", \"-version\"])\n",
    "    ffmpeg[\"version_line\"] = v.splitlines()[0] if isinstance(v, str) and v else v\n",
    "\n",
    "# Disk\n",
    "disk = {\n",
    "    \"week11_hw\": disk_usage(W11),\n",
    "    \"data_ro\": disk_usage(DATA_RO),\n",
    "    \"models_ro\": disk_usage(MODELS_RO),\n",
    "}\n",
    "\n",
    "# ---------- Snapshot ----------\n",
    "snapshot = {\n",
    "    \"env\": env,\n",
    "    \"dirs_status\": dirs_status,\n",
    "    \"models_present\": model_files,\n",
    "    \"packages\": versions,\n",
    "    \"cuda\": cuda,\n",
    "    \"ffmpeg\": ffmpeg,\n",
    "    \"disk\": disk,\n",
    "}\n",
    "\n",
    "SNAPSHOT.write_text(json.dumps(snapshot, indent=2))\n",
    "print(\"✅ Wrote probe snapshot ->\", SNAPSHOT)\n",
    "\n",
    "# ---------- Pretty print summary ----------\n",
    "def hr(): print(\"-\"*72)\n",
    "\n",
    "print(\"\\nWeek 11 Probe Summary\")\n",
    "hr()\n",
    "print(\"Root:\", env[\"paths\"][\"W11\"])\n",
    "print(\"Models (RO):\", env[\"paths\"][\"MODELS_RO\"])\n",
    "print(\"Data   (RO):\", env[\"paths\"][\"DATA_RO\"])\n",
    "hr()\n",
    "missing = [p for p,(exists, kind) in dirs_status.items() if not exists]\n",
    "print(f\"Dirs OK: {len(dirs_status)-len(missing)}/{len(dirs_status)}\")\n",
    "if missing:\n",
    "    print(\"Missing:\")\n",
    "    for m in missing: print(\"  -\", m)\n",
    "else:\n",
    "    print(\"All expected dirs present.\")\n",
    "hr()\n",
    "print(\"Model candidates (by hint):\")\n",
    "for h, files in model_files.items():\n",
    "    print(f\"  {h}: {len(files)} files\")\n",
    "    for f in files[:5]:\n",
    "        print(\"    -\", f)\n",
    "    if len(files) > 5: print(\"    ...\")\n",
    "hr()\n",
    "print(\"Key packages:\")\n",
    "for k in [\"torch\",\"torchvision\",\"torchaudio\",\"transformers\",\"accelerate\",\"bitsandbytes\",\"cv2\",\"imageio\",\"imageio_ffmpeg\",\"PIL\",\"pandas\",\"numpy\",\"networkx\",\"faiss\",\"faiss_cpu\",\"chromadb\"]:\n",
    "    print(f\"  {k:14s} -> {versions.get(k)}\")\n",
    "hr()\n",
    "print(\"CUDA/GPU:\")\n",
    "for k,v in cuda.items(): print(f\"  {k:14s}: {v}\")\n",
    "hr()\n",
    "print(\"ffmpeg:\")\n",
    "for k,v in ffmpeg.items(): print(f\"  {k:14s}: {v}\")\n",
    "hr()\n",
    "print(\"Disk (GB):\")\n",
    "for k,v in disk.items(): print(f\"  {k:14s}: {v}\")\n",
    "hr()\n",
    "\n",
    "# Return a compact dict for quick glance in Jupyter\n",
    "{\n",
    "    \"dirs_ok\": len(missing)==0,\n",
    "    \"have_qwen\": bool(model_files.get(\"Qwen2.5-VL-2B-Instruct\")),\n",
    "    \"have_intern\": bool(model_files.get(\"Intern3_5-VL-4B-Instruct\")),\n",
    "    \"cuda\": cuda.get(\"available\"),\n",
    "    \"ffmpeg\": bool(ffmpeg[\"path\"]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fdf3f0-af98-4ef8-82e0-470d8613562e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EOF marker not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph-RAG bootstrap complete\n",
      "PDFs scanned     : 17\n",
      "Docs indexed     : 17\n",
      "Graph nodes/edges: 4795/151113\n",
      "Outputs:\n",
      "  - raw_text_dir: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/raw_text\n",
      "  - graph_json: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/graph/graph.json\n",
      "  - graph_gml: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/graph/graph.gml\n",
      "  - log: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/logs/ingest.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pdfs_scanned': 17, 'nodes': 4795, 'edges': 151113}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 · Cell 1 — Graph-RAG Bootstrap (robust PDF ingest + simple graph)\n",
    "# Safe to re-run; idempotent by default.\n",
    "# Outputs under: artifacts/graph/{raw_text,graph,index,logs}\n",
    "\n",
    "import os, re, json, shutil, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "import networkx as nx  # present per probe\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "DATA_RO = Path(\"/home/manny-buff/projects/capstone/hw-rag/data\")  # PDFs live here\n",
    "GART = W11 / \"artifacts\" / \"graph\"\n",
    "RAW_TEXT = GART / \"raw_text\"\n",
    "GRAPH_DIR = GART / \"graph\"\n",
    "INDEX_DIR = GART / \"index\"\n",
    "LOGS_DIR = GART / \"logs\"\n",
    "LOG_FILE = LOGS_DIR / \"ingest.log\"\n",
    "\n",
    "# Source selection (adjust as needed)\n",
    "FILE_GLOBS = [\"**/*.pdf\"]        # scan all PDFs under DATA_RO\n",
    "MAX_PDFS = 20                    # cap to avoid huge runs on first pass; set None to disable\n",
    "MAX_PAGES_PER_DOC = 80           # cap per problematic doc; tweak later\n",
    "\n",
    "# Fault tolerance\n",
    "MIN_CHARS_PER_PAGE = 50          # pages below this are considered \"empty\" and trigger fallback\n",
    "USE_DELETE_PREVIOUS = True       # full refresh of text outputs/graph when re-running\n",
    "\n",
    "# -------------------- UTILS ---------------------\n",
    "def ensure_dirs():\n",
    "    for p in [RAW_TEXT, GRAPH_DIR, INDEX_DIR, LOGS_DIR]:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def reset_outputs():\n",
    "    if USE_DELETE_PREVIOUS:\n",
    "        for p in [RAW_TEXT, GRAPH_DIR, INDEX_DIR]:\n",
    "            if p.exists():\n",
    "                shutil.rmtree(p)\n",
    "        ensure_dirs()\n",
    "\n",
    "def log(msg):\n",
    "    LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.now().isoformat()}] {msg}\\n\")\n",
    "\n",
    "def which(cmd):\n",
    "    try:\n",
    "        from shutil import which as _which\n",
    "        return _which(cmd)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def run_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output(args, stderr=subprocess.STDOUT).decode(\"utf-8\", \"ignore\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def import_optional(*names):\n",
    "    mods = {}\n",
    "    for n in names:\n",
    "        try:\n",
    "            mods[n] = __import__(n)\n",
    "        except Exception:\n",
    "            mods[n] = None\n",
    "    return mods\n",
    "\n",
    "mods = import_optional(\"pypdf\", \"PyPDF2\", \"pdfminer\", \"pdfminer.high_level\")\n",
    "\n",
    "# -------------------- PDF EXTRACTION ---------------------\n",
    "def extract_with_pypdf(pdf_path: Path):\n",
    "    text_pages = []\n",
    "    try:\n",
    "        if mods[\"pypdf\"]:\n",
    "            reader = mods[\"pypdf\"].PdfReader(str(pdf_path))\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    t = page.extract_text() or \"\"\n",
    "                except Exception:\n",
    "                    t = \"\"\n",
    "                text_pages.append(t)\n",
    "        elif mods[\"PyPDF2\"]:\n",
    "            reader = mods[\"PyPDF2\"].PdfReader(str(pdf_path))\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    t = page.extract_text() or \"\"\n",
    "                except Exception:\n",
    "                    t = \"\"\n",
    "                text_pages.append(t)\n",
    "        else:\n",
    "            return None\n",
    "        return text_pages\n",
    "    except Exception as e:\n",
    "        log(f\"pypdf/PyPDF2 failed for {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_with_pdfminer(pdf_path: Path):\n",
    "    try:\n",
    "        if mods[\"pdfminer\"] and mods[\"pdfminer.high_level\"]:\n",
    "            from pdfminer.high_level import extract_pages\n",
    "            from pdfminer.layout import LTTextContainer\n",
    "            pages = []\n",
    "            for pageno, page_layout in enumerate(extract_pages(str(pdf_path))):\n",
    "                chunks = []\n",
    "                for element in page_layout:\n",
    "                    if isinstance(element, LTTextContainer):\n",
    "                        chunks.append(element.get_text())\n",
    "                pages.append(\"\\n\".join(chunks))\n",
    "            return pages\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        log(f\"pdfminer failed for {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_with_pdftotext(pdf_path: Path):\n",
    "    if not which(\"pdftotext\"):\n",
    "        return None\n",
    "    try:\n",
    "        # Write to temp .txt; then split per form-feed if present\n",
    "        tmp_txt = RAW_TEXT / f\"__tmp_{pdf_path.stem}.txt\"\n",
    "        if tmp_txt.exists():\n",
    "            tmp_txt.unlink()\n",
    "        cmd = [\"pdftotext\", \"-layout\", str(pdf_path), str(tmp_txt)]\n",
    "        run = run_cmd(cmd)\n",
    "        if isinstance(run, str) and run.startswith(\"ERROR\"):\n",
    "            log(f\"pdftotext error for {pdf_path.name}: {run}\")\n",
    "            return None\n",
    "        txt = tmp_txt.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        # heuristic split: many pdftotext builds do not insert form-feeds, so fallback to dense split by page markers later\n",
    "        pages = txt.split(\"\\f\")\n",
    "        if len(pages) == 1:  # fallback: chunk by ~8000 chars to simulate pages\n",
    "            sz = 8000\n",
    "            pages = [txt[i:i+sz] for i in range(0, len(txt), sz)]\n",
    "        tmp_txt.unlink(missing_ok=True)\n",
    "        return pages\n",
    "    except Exception as e:\n",
    "        log(f\"pdftotext exception for {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_with_ocr(pdf_path: Path):\n",
    "    # Optional OCR using pdftoppm + tesseract if available\n",
    "    pdftoppm = which(\"pdftoppm\")\n",
    "    tesseract = which(\"tesseract\")\n",
    "    if not (pdftoppm and tesseract):\n",
    "        return None\n",
    "    try:\n",
    "        ocr_dir = RAW_TEXT / f\"__ocr_{pdf_path.stem}\"\n",
    "        if ocr_dir.exists():\n",
    "            shutil.rmtree(ocr_dir)\n",
    "        ocr_dir.mkdir(parents=True, exist_ok=True)\n",
    "        # Convert to PNG at moderate DPI to keep runtime sane\n",
    "        # NOTE: limit pages using -f / -l if MAX_PAGES_PER_DOC set\n",
    "        args = [pdftoppm, \"-png\", \"-r\", \"200\", str(pdf_path), str(ocr_dir / \"page\")]\n",
    "        run = run_cmd(args)\n",
    "        if isinstance(run, str) and run.startswith(\"ERROR\"):\n",
    "            log(f\"pdftoppm error for {pdf_path.name}: {run}\")\n",
    "            return None\n",
    "        pages = []\n",
    "        pngs = sorted(ocr_dir.glob(\"page-*.png\"))\n",
    "        if MAX_PAGES_PER_DOC:\n",
    "            pngs = pngs[:MAX_PAGES_PER_DOC]\n",
    "        for i, img in enumerate(pngs):\n",
    "            out_txt = ocr_dir / f\"page-{i:04d}.txt\"\n",
    "            cmd = [tesseract, str(img), str(out_txt.with_suffix(\"\")), \"--psm\", \"6\"]\n",
    "            _ = run_cmd(cmd)\n",
    "            if out_txt.with_suffix(\".txt\").exists():\n",
    "                pages.append(out_txt.with_suffix(\".txt\").read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        # Clean up images to save space\n",
    "        for p in pngs:\n",
    "            p.unlink(missing_ok=True)\n",
    "        return pages if pages else None\n",
    "    except Exception as e:\n",
    "        log(f\"OCR exception for {pdf_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def robust_extract(pdf_path: Path):\n",
    "    # 1) pypdf/PyPDF2\n",
    "    pages = extract_with_pypdf(pdf_path)\n",
    "    # Check quality\n",
    "    def is_sparse(pages):\n",
    "        if not pages: return True\n",
    "        nonempty = [p for p in pages if p and len(p.strip()) >= MIN_CHARS_PER_PAGE]\n",
    "        return len(nonempty) == 0\n",
    "    if pages is None or is_sparse(pages):\n",
    "        # 2) pdfminer.six\n",
    "        pm = extract_with_pdfminer(pdf_path)\n",
    "        if pm and not is_sparse(pm):\n",
    "            pages = pm\n",
    "        else:\n",
    "            # 3) pdftotext\n",
    "            p2t = extract_with_pdftotext(pdf_path)\n",
    "            if p2t and not is_sparse(p2t):\n",
    "                pages = p2t\n",
    "            else:\n",
    "                # 4) OCR\n",
    "                ocr = extract_with_ocr(pdf_path)\n",
    "                if ocr and not is_sparse(ocr):\n",
    "                    pages = ocr\n",
    "                else:\n",
    "                    pages = pages if pages else []\n",
    "\n",
    "    # Truncate if too long\n",
    "    if MAX_PAGES_PER_DOC and len(pages) > MAX_PAGES_PER_DOC:\n",
    "        pages = pages[:MAX_PAGES_PER_DOC]\n",
    "    return pages\n",
    "\n",
    "# -------------------- ENTITY & GRAPH ---------------------\n",
    "# Simple domain-ish term harvesting (no external NLP deps)\n",
    "TOOL_PAT = re.compile(r\"\\b(hammer|screwdriver|drill|level|saw|wrench|pliers|sander|trowel|chisel|stud\\s?finder|tape\\s?measure|shop\\s?vac|utility\\s?knife)\\b\", re.I)\n",
    "MATERIAL_PAT = re.compile(r\"\\b(drywall|joint\\s?compound|spackle|plywood|stud|2x4|adhesive|primer|paint|sealant|caulk|screw|nail|bolt|washer|bracket|shim)\\b\", re.I)\n",
    "MEASURE_PAT = re.compile(r\"\\b(\\d+(\\.\\d+)?\\s?(mm|cm|m|in(?:ch(?:es)?)?|ft|feet|yd|g|kg|lb|lbs|oz))\\b\", re.I)\n",
    "\n",
    "def extract_entities(text):\n",
    "    ents = set()\n",
    "    for m in TOOL_PAT.finditer(text): ents.add((\"tool\", m.group(0).lower()))\n",
    "    for m in MATERIAL_PAT.finditer(text): ents.add((\"material\", m.group(0).lower()))\n",
    "    for m in MEASURE_PAT.finditer(text): ents.add((\"measure\", m.group(0).lower()))\n",
    "    # Heuristic extra: capture Title-case multi-words (likely components/proper nouns)\n",
    "    for m in re.finditer(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})\\b\", text):\n",
    "        if len(m.group(0)) >= 4:\n",
    "            ents.add((\"term\", m.group(0)))\n",
    "    return ents\n",
    "\n",
    "def build_graph(doc_records):\n",
    "    G = nx.Graph()\n",
    "    # Add doc + page nodes; connect mentions; co-occurrence edges among entities per page\n",
    "    for rec in doc_records:\n",
    "        doc_id = f\"doc::{rec['doc_name']}\"\n",
    "        if doc_id not in G: G.add_node(doc_id, kind=\"document\", path=rec[\"doc_path\"])\n",
    "        for page in rec[\"pages\"]:\n",
    "            page_id = f\"{doc_id}::p{page['page_index']}\"\n",
    "            if page_id not in G: G.add_node(page_id, kind=\"page\", text_len=len(page['text']))\n",
    "            G.add_edge(doc_id, page_id, kind=\"has_page\")\n",
    "            # entities\n",
    "            ent_nodes = []\n",
    "            for etype, eval_ in page[\"entities\"]:\n",
    "                ent_id = f\"ent::{etype}::{eval_}\"\n",
    "                if ent_id not in G: G.add_node(ent_id, kind=\"entity\", etype=etype, value=eval_)\n",
    "                G.add_edge(page_id, ent_id, kind=\"mentions\")\n",
    "                ent_nodes.append(ent_id)\n",
    "            # co-occurrence among entities on the same page\n",
    "            for i in range(len(ent_nodes)):\n",
    "                for j in range(i+1, len(ent_nodes)):\n",
    "                    a, b = sorted([ent_nodes[i], ent_nodes[j]])\n",
    "                    if G.has_edge(a, b):\n",
    "                        G[a][b][\"weight\"] = G[a][b].get(\"weight\", 1) + 1\n",
    "                    else:\n",
    "                        G.add_edge(a, b, kind=\"cooccur\", weight=1)\n",
    "    return G\n",
    "\n",
    "# -------------------- MAIN ---------------------\n",
    "reset_outputs()\n",
    "ensure_dirs()\n",
    "\n",
    "pdfs = []\n",
    "for glob in FILE_GLOBS:\n",
    "    pdfs.extend(sorted(DATA_RO.glob(glob)))\n",
    "if MAX_PDFS:\n",
    "    pdfs = pdfs[:MAX_PDFS]\n",
    "\n",
    "doc_records = []\n",
    "for pdf in pdfs:\n",
    "    try:\n",
    "        log(f\"Extracting: {pdf}\")\n",
    "        pages = robust_extract(pdf)\n",
    "        # write raw pages\n",
    "        out_dir = RAW_TEXT / pdf.stem\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        kept = 0\n",
    "        for i, t in enumerate(pages):\n",
    "            if t and len(t.strip()) >= MIN_CHARS_PER_PAGE:\n",
    "                (out_dir / f\"page_{i:04d}.txt\").write_text(t, encoding=\"utf-8\", errors=\"ignore\")\n",
    "                kept += 1\n",
    "        rec = {\n",
    "            \"doc_name\": pdf.name,\n",
    "            \"doc_path\": str(pdf),\n",
    "            \"pages\": []\n",
    "        }\n",
    "        for i, t in enumerate(pages):\n",
    "            if not t or len(t.strip()) < MIN_CHARS_PER_PAGE:\n",
    "                continue\n",
    "            ents = list(extract_entities(t))\n",
    "            rec[\"pages\"].append({\n",
    "                \"page_index\": i,\n",
    "                \"text\": t[:3000],  # keep snippet for provenance\n",
    "                \"entities\": ents\n",
    "            })\n",
    "        doc_records.append(rec)\n",
    "        log(f\"Done: {pdf.name} pages_kept={kept} ents_total={sum(len(p['entities']) for p in rec['pages'])}\")\n",
    "    except Exception as e:\n",
    "        log(f\"FAIL: {pdf} -> {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "# Build & save graph\n",
    "G = build_graph(doc_records)\n",
    "\n",
    "graph_json = GRAPH_DIR / \"graph.json\"\n",
    "graph_gml  = GRAPH_DIR / \"graph.gml\"\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# JSON export\n",
    "with open(graph_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    nodes = [{\"id\": n, **G.nodes[n]} for n in G.nodes]\n",
    "    edges = [{\"u\": u, \"v\": v, **G[u][v]} for u, v in G.edges]\n",
    "    json.dump({\"nodes\": nodes, \"edges\": edges}, f, indent=2)\n",
    "# GML export (useful for Gephi, yEd)\n",
    "nx.write_gml(G, graph_gml)\n",
    "\n",
    "# Small manifest for quick reference\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"data_root\": str(DATA_RO),\n",
    "    \"pdf_count_scanned\": len(pdfs),\n",
    "    \"docs_indexed\": len(doc_records),\n",
    "    \"nodes\": G.number_of_nodes(),\n",
    "    \"edges\": G.number_of_edges(),\n",
    "    \"outputs\": {\n",
    "        \"raw_text_dir\": str(RAW_TEXT),\n",
    "        \"graph_json\": str(graph_json),\n",
    "        \"graph_gml\": str(graph_gml),\n",
    "        \"log\": str(LOG_FILE),\n",
    "    }\n",
    "}\n",
    "(GRAPH_DIR / \"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "\n",
    "# Console summary\n",
    "print(\"✅ Graph-RAG bootstrap complete\")\n",
    "print(f\"PDFs scanned     : {manifest['pdf_count_scanned']}\")\n",
    "print(f\"Docs indexed     : {manifest['docs_indexed']}\")\n",
    "print(f\"Graph nodes/edges: {manifest['nodes']}/{manifest['edges']}\")\n",
    "print(\"Outputs:\")\n",
    "for k,v in manifest[\"outputs\"].items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# Return a tiny dict for your notebook sidebar\n",
    "{\"pdfs_scanned\": manifest[\"pdf_count_scanned\"], \"nodes\": manifest[\"nodes\"], \"edges\": manifest[\"edges\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a938b877-35e3-409c-a6f1-59862ee14398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query layer ready. Built/loaded index with 696 pages. Index file -> /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/index/tfidf_index.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1 · Cell 2 — Graph-RAG Query Layer (page TF-IDF + graph entities)\n",
    "# - No new installs; pure-Python TF-IDF.\n",
    "# - Reads: artifacts/graph/graph/graph.json and artifacts/graph/raw_text/<stem>/page_*.txt\n",
    "# - Writes: artifacts/graph/index/tfidf_index.json\n",
    "# - Provides: search_pages(query, top_k=5) -> list of dicts {doc, page, score, snippet, entities, path}\n",
    "\n",
    "import json, math, re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "GRAPH_JSON = W11 / \"artifacts\" / \"graph\" / \"graph\" / \"graph.json\"\n",
    "RAW_TEXT   = W11 / \"artifacts\" / \"graph\" / \"raw_text\"\n",
    "INDEX_DIR  = W11 / \"artifacts\" / \"graph\" / \"index\"\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TFIDF_INDEX = INDEX_DIR / \"tfidf_index.json\"\n",
    "\n",
    "# Optional quick demo (set a string to run a test search at the bottom)\n",
    "TEST_QUERY = None  # e.g., \"seal a drywall seam with joint compound\"\n",
    "\n",
    "# ----------------- HELPERS -----------------\n",
    "WORD = re.compile(r\"[a-zA-Z0-9]+(?:'[a-z0-9]+)?\")\n",
    "STOP = set(\"\"\"\n",
    "a an and are as at be by for from has have in is it its of on or that the to with your you we he she they them their our\n",
    "\"\"\".split())\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return [w.lower() for w in WORD.findall(text) if w.lower() not in STOP and len(w) > 1]\n",
    "\n",
    "def read_text_safe(path: Path, max_chars=20000):\n",
    "    try:\n",
    "        t = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        t = \"\"\n",
    "    return t[:max_chars]\n",
    "\n",
    "def parse_page_id(page_node_id: str):\n",
    "    # format from bootstrap: \"doc::<DOCNAME>::p<index>\"\n",
    "    try:\n",
    "        _, docname, p = page_node_id.split(\"::\")\n",
    "        if p.startswith(\"p\"):\n",
    "            page_idx = int(p[1:])\n",
    "        else:\n",
    "            page_idx = int(p)\n",
    "        stem = docname.rsplit(\".\", 1)[0]\n",
    "        return docname, stem, page_idx\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "def snippet_with_hits(text, hits, width=240):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # show beginning by default; bold known hits (case-insensitive)\n",
    "    s = text[:width]\n",
    "    for h in sorted(set(hits), key=len, reverse=True):\n",
    "        s = re.sub(rf\"(?i)\\b({re.escape(h)})\\b\", r\"**\\1**\", s)\n",
    "    return s\n",
    "\n",
    "# ----------------- LOAD GRAPH -----------------\n",
    "if not GRAPH_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Missing graph json at {GRAPH_JSON}\")\n",
    "\n",
    "g = json.loads(GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "nodes = {n[\"id\"]: n for n in g[\"nodes\"]}\n",
    "adj = defaultdict(list)\n",
    "for e in g[\"edges\"]:\n",
    "    u, v = e[\"u\"], e[\"v\"]\n",
    "    adj[u].append((v, e))\n",
    "    adj[v].append((u, e))\n",
    "\n",
    "# Collect page nodes and resolve their raw-text file paths\n",
    "pages = []  # list of dicts with keys: id, doc, stem, page_idx, path, entities\n",
    "for nid, nd in nodes.items():\n",
    "    if nd.get(\"kind\") != \"page\":\n",
    "        continue\n",
    "    doc, stem, pidx = parse_page_id(nid)\n",
    "    if doc is None:\n",
    "        continue\n",
    "    # reconstruct expected text path\n",
    "    text_path = RAW_TEXT / stem / f\"page_{pidx:04d}.txt\"\n",
    "    # gather entity neighbors from graph\n",
    "    ents = []\n",
    "    for nbr, eprops in adj[nid]:\n",
    "        nbn = nodes.get(nbr, {})\n",
    "        if nbn.get(\"kind\") == \"entity\":\n",
    "            ents.append({\"etype\": nbn.get(\"etype\"), \"value\": nbn.get(\"value\")})\n",
    "    pages.append({\"id\": nid, \"doc\": doc, \"stem\": stem, \"page_idx\": pidx, \"path\": text_path, \"entities\": ents})\n",
    "\n",
    "# Filter pages to those that actually have text files\n",
    "pages = [p for p in pages if p[\"path\"].exists()]\n",
    "\n",
    "# ----------------- BUILD / LOAD TF-IDF -----------------\n",
    "def build_index(pages):\n",
    "    docs_tokens = []\n",
    "    df = Counter()\n",
    "    for p in pages:\n",
    "        t = read_text_safe(p[\"path\"])\n",
    "        toks = tokenize(t)\n",
    "        docs_tokens.append(toks)\n",
    "        for term in set(toks):\n",
    "            df[term] += 1\n",
    "\n",
    "    N = len(docs_tokens)\n",
    "    idf = {term: math.log((1 + N) / (1 + dfv)) + 1.0 for term, dfv in df.items()}  # smoothed\n",
    "    doc_tfs = []\n",
    "    norms = []\n",
    "    for toks in docs_tokens:\n",
    "        tf = Counter(toks)\n",
    "        # L2 norm of tf-idf vector\n",
    "        sq = 0.0\n",
    "        for term, c in tf.items():\n",
    "            w = (c / max(1, len(toks))) * idf.get(term, 0.0)\n",
    "            sq += w * w\n",
    "        norms.append(math.sqrt(sq) or 1.0)\n",
    "        doc_tfs.append(tf)\n",
    "\n",
    "    # Persist compactly\n",
    "    index = {\n",
    "        \"built_at\": datetime.now().isoformat(),\n",
    "        \"N\": N,\n",
    "        \"idf\": idf,                 # {term: idf}\n",
    "        \"norms\": norms,             # [float]\n",
    "        \"docs\": [                   # align with pages[]\n",
    "            {\"doc\": pages[i][\"doc\"], \"stem\": pages[i][\"stem\"], \"page_idx\": pages[i][\"page_idx\"]}\n",
    "            for i in range(N)\n",
    "        ],\n",
    "    }\n",
    "    # To keep size reasonable, store only the top 200 terms by tf for each doc\n",
    "    topk = 200\n",
    "    index[\"tf_top\"] = []\n",
    "    for i, tf in enumerate(doc_tfs):\n",
    "        index[\"tf_top\"].append(dict(tf.most_common(topk)))\n",
    "    return index\n",
    "\n",
    "def load_index():\n",
    "    if TFIDF_INDEX.exists():\n",
    "        try:\n",
    "            return json.loads(TFIDF_INDEX.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "    idx = build_index(pages)\n",
    "    TFIDF_INDEX.write_text(json.dumps(idx), encoding=\"utf-8\")\n",
    "    return idx\n",
    "\n",
    "index = load_index()\n",
    "\n",
    "# ----------------- SEARCH -----------------\n",
    "def search_pages(query: str, top_k: int = 5):\n",
    "    q_toks = tokenize(query)\n",
    "    if not q_toks:\n",
    "        return []\n",
    "    # Query tf-idf\n",
    "    q_tf = Counter(q_toks)\n",
    "    q_vec = {}\n",
    "    for term, c in q_tf.items():\n",
    "        w = (c / max(1, len(q_toks))) * index[\"idf\"].get(term, 0.0)\n",
    "        if w > 0: q_vec[term] = w\n",
    "    q_norm = math.sqrt(sum(v*v for v in q_vec.values())) or 1.0\n",
    "\n",
    "    scores = []\n",
    "    N = index[\"N\"]\n",
    "    for i in range(N):\n",
    "        tf_top = index[\"tf_top\"][i]\n",
    "        # cosine similarity over intersection of query terms ∩ doc terms (top terms only)\n",
    "        dot = 0.0\n",
    "        for term, qw in q_vec.items():\n",
    "            if term in tf_top:\n",
    "                dw = (tf_top[term] / 200.0) * index[\"idf\"].get(term, 0.0)  # normalize by top cutoff\n",
    "                dot += qw * dw\n",
    "        denom = (q_norm * (index[\"norms\"][i] or 1.0))\n",
    "        s = dot / denom if denom else 0.0\n",
    "        if s > 0:\n",
    "            scores.append((s, i))\n",
    "    scores.sort(reverse=True)\n",
    "    hits = scores[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for score, i in hits:\n",
    "        meta = index[\"docs\"][i]\n",
    "        # find the corresponding page entry\n",
    "        # (find first match; our lists are aligned by build_index)\n",
    "        pg = next((p for p in pages if p[\"stem\"] == meta[\"stem\"] and p[\"page_idx\"] == meta[\"page_idx\"]), None)\n",
    "        if not pg:\n",
    "            continue\n",
    "        txt = read_text_safe(pg[\"path\"], max_chars=1200)\n",
    "        results.append({\n",
    "            \"score\": round(float(score), 6),\n",
    "            \"doc\": pg[\"doc\"],\n",
    "            \"page\": pg[\"page_idx\"],\n",
    "            \"snippet\": snippet_with_hits(txt, q_toks, width=260),\n",
    "            \"entities\": pg[\"entities\"][:12],  # first dozen entity mentions\n",
    "            \"path\": str(pg[\"path\"])\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# -------------- OPTIONAL QUICK DEMO --------------\n",
    "if TEST_QUERY:\n",
    "    print(f\"\\nQuery: {TEST_QUERY!r}\")\n",
    "    for r in search_pages(TEST_QUERY, top_k=5):\n",
    "        print(f\"- [{r['score']:.3f}] {r['doc']} p{r['page']} :: {r['path']}\")\n",
    "        print(f\"  ents: {[ (e['etype'], e['value']) for e in r['entities'][:6] ]}\")\n",
    "        print(f\"  {r['snippet']}\\n\")\n",
    "\n",
    "print(f\"✅ Query layer ready. Built/loaded index with {index['N']} pages. Index file -> {TFIDF_INDEX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8de883-76c5-4046-a0fd-b66c5d917adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Context pack created\n",
      "Query: How do I fix a faucet leak?\n",
      "Results: 5 pages\n",
      "Saved:\n",
      "  - /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/queries/20251103_112405_faucet_leak.json\n",
      "  - /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/queries/20251103_112405_faucet_leak_context.txt\n",
      "\n",
      "Preview (first ~20 lines):\n",
      "You are a precise home-repair assistant. Use only the provided snippets as primary evidence.\n",
      "Cite the snippet index like [1], [2] when relevant, and give step-by-step, tool- and material-aware instructions.\n",
      "\n",
      "=== SNIPPETS ===\n",
      "[1] doc=1001 do-it-yourself hints & tips  tricks.pdf page=13 score=0.304864\n",
      "    ents: term:Also, term:Fold, term:Gradually, term:High, term:Never, term:Pmpomt, term:Preventing, term:Shut, term:Tack, term:This, term:Turn, term:When\n",
      "    path: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/raw_text/1001 do-it-yourself hints & tips  tricks/page_0013.txt\n",
      "    text: SAFE  AND  SNART  If  you  have  a  flood,  mini-  mize the  damage  with  these  **do**'s  and  don'ts.  >■  Shut  off  the  main  water  valve  if  the  flood  is  a  result  of  a  broken  pipe  >■  Shut  ofi'  electricity  to  the  flooded  area— if  it  is  p\n",
      "\n",
      "[2] doc=the-complete-idiots-guide-to-simple-home-repair.pdf page=12 score=0.22597\n",
      "    ents: term:Bathtubs, term:Beware, term:Cleaning, term:Drain, term:Faucet, term:Fixing, term:Plunge, term:Removing, term:Repairing, term:Replacing, term:Sink, term:Whe\n",
      "    path: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/raw_text/the-complete-idiots-guide-to-simple-home-repair/page_0012.txt\n",
      "    text: 1]\\bS\\ba fW >O`b\u000e!(\u000e BVS\u000e1W`QcZOb]`g\u000eAgabS[(\u000e>Zc[PW\\U\u000eO\\R\u000e4Wfbc`Sa\u000e \u001f#\u001f \u000e \u001f\n",
      "\u000e A]ZdW\\U\u000e>W^S\u000e>`]PZS[a\u000e \u001f#! Small Leaks: Emergency **Fix**......................................................154 Thawing Frozen Pipes ..................................................\n",
      "\n",
      "[3] doc=1001 do-it-yourself hints & tips  tricks.pdf page=46 score=0.221741\n",
      "    ents: material:nail, material:paint, term:Cover, term:Extend, term:Glue, term:Head, term:Locate, term:Replace, term:Then, term:These, term:They, tool:hammer\n",
      "    path: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph/raw_text/1001 do-it-yourself hints & tips  tricks/page_0046.txt\n",
      "    text: Seen  from  afar.  Inspect  the  roof  in  spring  and  fall  and  after  severe  stomis.  A  quick  inspection  with  binoculars  often  does  the  trick.  For  a  close  look,  raise  a  ladder  and  inspect  the  surface  from  the  eaves.  Pay  special  at\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'saved_json': '/home/manny-buff/projects/capstone/week11-hw/artifacts/graph/queries/20251103_112405_faucet_leak.json',\n",
       " 'saved_txt': '/home/manny-buff/projects/capstone/week11-hw/artifacts/graph/queries/20251103_112405_faucet_leak_context.txt',\n",
       " 'hits': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 · Cell 3 — Context packer for Qwen / Intern (Home Repair query)\n",
    "# Uses search_pages() from Cell 2 to retrieve graph-aware page snippets.\n",
    "# Saves a JSON + TXT pack and prints a preview.\n",
    "\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "QUERY_DIR = W11 / \"artifacts\" / \"graph\" / \"queries\"\n",
    "QUERY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "USER_QUERY = \"How do I fix a faucet leak?\"\n",
    "TOP_K = 5\n",
    "\n",
    "# ---------- Retrieve ----------\n",
    "try:\n",
    "    results = search_pages(USER_QUERY, top_k=TOP_K)\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"search_pages() not found. Run Step 1 · Cell 2 first.\") from e\n",
    "\n",
    "# ---------- Build context block ----------\n",
    "def format_result(i, r):\n",
    "    ents = \", \".join(sorted({f\"{e['etype']}:{e['value']}\" for e in r[\"entities\"]}))[:160]\n",
    "    snippet = r[\"snippet\"].replace(\"\\n\", \" \").strip()\n",
    "    return (\n",
    "        f\"[{i}] doc={r['doc']} page={r['page']} score={r['score']}\\n\"\n",
    "        f\"    ents: {ents}\\n\"\n",
    "        f\"    path: {r['path']}\\n\"\n",
    "        f\"    text: {snippet}\\n\"\n",
    "    )\n",
    "\n",
    "context_lines = []\n",
    "for i, r in enumerate(results, 1):\n",
    "    context_lines.append(format_result(i, r))\n",
    "\n",
    "context_block = (\n",
    "    \"You are a precise home-repair assistant. Use only the provided snippets as primary evidence.\\n\"\n",
    "    \"Cite the snippet index like [1], [2] when relevant, and give step-by-step, tool- and material-aware instructions.\\n\\n\"\n",
    "    \"=== SNIPPETS ===\\n\" + \"\\n\".join(context_lines)\n",
    ")\n",
    "\n",
    "# ---------- Qwen-style messages (text-only for now) ----------\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful home-repair assistant. \"\n",
    "            \"Be concise, safe, and practical. Include tool lists, safety notes, and sequencing. \"\n",
    "            \"When uncertain, state assumptions and alternatives. Cite snippet IDs like [1], [2].\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Question: {USER_QUERY}\\n\\n\"\n",
    "            f\"{context_block}\\n\\n\"\n",
    "            \"Answer with a short checklist of steps and a brief explanation.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- Save pack ----------\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "stem = f\"{ts}_faucet_leak\"\n",
    "\n",
    "pack = {\n",
    "    \"created_at\": ts,\n",
    "    \"query\": USER_QUERY,\n",
    "    \"top_k\": TOP_K,\n",
    "    \"results\": results,          # full provenance\n",
    "    \"context_block\": context_block,\n",
    "    \"messages\": messages,\n",
    "}\n",
    "\n",
    "json_path = QUERY_DIR / f\"{stem}.json\"\n",
    "txt_path  = QUERY_DIR / f\"{stem}_context.txt\"\n",
    "\n",
    "json_path.write_text(json.dumps(pack, indent=2), encoding=\"utf-8\")\n",
    "txt_path.write_text(context_block, encoding=\"utf-8\")\n",
    "\n",
    "# ---------- Preview ----------\n",
    "print(\"✅ Context pack created\")\n",
    "print(\"Query:\", USER_QUERY)\n",
    "print(f\"Results: {len(results)} pages\")\n",
    "print(\"Saved:\")\n",
    "print(\"  -\", json_path)\n",
    "print(\"  -\", txt_path)\n",
    "print(\"\\nPreview (first ~20 lines):\")\n",
    "print(\"\\n\".join(context_block.splitlines()[:20]))\n",
    "\n",
    "# Compact dict for notebook display\n",
    "{\n",
    "    \"saved_json\": str(json_path),\n",
    "    \"saved_txt\": str(txt_path),\n",
    "    \"hits\": len(results)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135d6d8d-8056-488c-8455-24d7f892510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /home/manny-buff/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 330M/330M [00:01<00:00, 275MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph-RAG v2 built\n",
      "Docs: 17\n",
      "Images total: 388 | embedded: 388 | skipped: 0\n",
      "Embedding backend: torchvision_vit_b_16\n",
      "Saved:\n",
      "  - /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/graph/graph.json\n",
      "  - /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/graph/graph.gml\n",
      "  - /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1 · Cell X — Graph-RAG v2: PDF images + image embeddings + new graph\n",
    "# Safe to re-run; creates artifacts/graph_v2/* without touching v1.\n",
    "# Requires: networkx, PIL, torch (preferred), and optionally open_clip or torchvision for embeddings.\n",
    "# External tools (auto-detected): pdfimages (preferred), or pdftoppm fallback.\n",
    "\n",
    "import os, re, json, math, shutil, subprocess, traceback\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Paths ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "V1_GRAPH_JSON = W11 / \"artifacts\" / \"graph\" / \"graph\" / \"graph.json\"\n",
    "\n",
    "V2_ROOT = W11 / \"artifacts\" / \"graph_v2\"\n",
    "V2_IMG  = V2_ROOT / \"images\"\n",
    "V2_EMB  = V2_ROOT / \"embeddings\"\n",
    "V2_LOGS = V2_ROOT / \"logs\"\n",
    "V2_GRAPH_DIR = V2_ROOT / \"graph\"\n",
    "V2_GRAPH_JSON = V2_GRAPH_DIR / \"graph.json\"\n",
    "V2_GRAPH_GML  = V2_GRAPH_DIR / \"graph.gml\"\n",
    "V2_MANIFEST   = V2_ROOT / \"manifest.json\"\n",
    "V2_LOGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Config ----------\n",
    "MAX_PDFS = None          # None = all docs from v1. Set small int to throttle.\n",
    "MAX_IMAGES_PER_DOC = 40  # hard cap per document\n",
    "MIN_W, MIN_H = 64, 64    # skip tiny images\n",
    "EMB_FLOATS = 6           # decimals in json fallback\n",
    "USE_FRESH = True         # wipe V2 images/embeddings/graph on re-run\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def log(msg):\n",
    "    with open(V2_LOGS / \"build_v2.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.now().isoformat()}] {msg}\\n\")\n",
    "\n",
    "def which(cmd):\n",
    "    try:\n",
    "        from shutil import which as _w\n",
    "        return _w(cmd)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def run_cmd(args):\n",
    "    try:\n",
    "        return subprocess.check_output(args, stderr=subprocess.STDOUT).decode(\"utf-8\",\"ignore\")\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def parse_page_id(page_node_id: str):\n",
    "    # v1 used: \"doc::<DOCNAME>::p<index>\"\n",
    "    try:\n",
    "        _, docname, p = page_node_id.split(\"::\")\n",
    "        pidx = int(p[1:]) if p.startswith(\"p\") else int(p)\n",
    "        stem = docname.rsplit(\".\",1)[0]\n",
    "        return docname, stem, pidx\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "# ---------- Prep ----------\n",
    "if USE_FRESH:\n",
    "    for p in [V2_IMG, V2_EMB, V2_GRAPH_DIR]:\n",
    "        if p.exists(): shutil.rmtree(p)\n",
    "for p in [V2_IMG, V2_EMB, V2_GRAPH_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not V1_GRAPH_JSON.exists():\n",
    "    raise FileNotFoundError(f\"v1 graph not found at {V1_GRAPH_JSON}\")\n",
    "\n",
    "v1 = json.loads(V1_GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "nodes_v1 = {n[\"id\"]: n for n in v1[\"nodes\"]}\n",
    "edges_v1 = v1[\"edges\"]\n",
    "\n",
    "# Build a quick map: document -> path, pages for that doc\n",
    "doc_to_path = {}\n",
    "doc_to_pages = {}\n",
    "for nid, nd in nodes_v1.items():\n",
    "    if nd.get(\"kind\") == \"document\":\n",
    "        # in v1 we stored 'path' on the document node\n",
    "        doc_to_path[nid] = nd.get(\"path\")\n",
    "        doc_to_pages[nid] = []\n",
    "for nid, nd in nodes_v1.items():\n",
    "    if nd.get(\"kind\") == \"page\":\n",
    "        # find doc node by splitting id\n",
    "        docname, stem, pidx = parse_page_id(nid)\n",
    "        if docname is None: \n",
    "            continue\n",
    "        doc_id = f\"doc::{docname}\"\n",
    "        if doc_id in doc_to_pages:\n",
    "            doc_to_pages[doc_id].append((nid, pidx))\n",
    "\n",
    "# ---------- Image extraction backends ----------\n",
    "HAVE_PDFIMAGES = bool(which(\"pdfimages\"))\n",
    "HAVE_PDFTOPPM  = bool(which(\"pdftoppm\"))\n",
    "\n",
    "def extract_images_pdfimages(pdf_path: Path, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Use poppler pdfimages to extract embedded images with page numbers.\n",
    "    Returns: list of dicts: {path, page, width, height}\n",
    "    \"\"\"\n",
    "    out_prefix = out_dir / \"img\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Map images -> pages from `pdfimages -list`\n",
    "    listing = run_cmd([\"pdfimages\", \"-list\", str(pdf_path)])\n",
    "    page_map = []  # [(page, idx)]\n",
    "    for line in listing.splitlines():\n",
    "        # Skip headers; look for lines starting with page number\n",
    "        if not line.strip() or not line.strip()[0].isdigit():\n",
    "            continue\n",
    "        parts = re.split(r\"\\s+\", line.strip())\n",
    "        try:\n",
    "            page = int(parts[0])\n",
    "            num  = int(parts[1])\n",
    "        except Exception:\n",
    "            continue\n",
    "        page_map.append((page, num))\n",
    "\n",
    "    # Extract as PNG with page numbers in filenames (-p)\n",
    "    _ = run_cmd([\"pdfimages\", \"-png\", \"-p\", str(pdf_path), str(out_prefix)])\n",
    "\n",
    "    # Collect files; filenames usually include '-<page>-<num>.png'\n",
    "    imgs = sorted(out_dir.glob(\"img-*-*.png\"))\n",
    "    results = []\n",
    "    for p in imgs:\n",
    "        m = re.search(r\"img-(\\d+)-(\\d+)\\.png$\", p.name)\n",
    "        if m:\n",
    "            page = int(m.group(1))\n",
    "        else:\n",
    "            # fallback if filename pattern differs: try align with page_map order\n",
    "            idx = len(results)\n",
    "            page = page_map[idx][0] if idx < len(page_map) else -1\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                w, h = im.size\n",
    "        except Exception:\n",
    "            w, h = 0, 0\n",
    "        results.append({\"path\": p, \"page\": page, \"width\": w, \"height\": h})\n",
    "    return results\n",
    "\n",
    "def extract_images_pdftoppm(pdf_path: Path, out_dir: Path, dpi=150):\n",
    "    \"\"\"\n",
    "    Render each page as an image (PNG) when embedded image extraction isn't available.\n",
    "    Returns: list of dicts: {path, page, width, height}\n",
    "    \"\"\"\n",
    "    out_prefix = out_dir / \"page\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    _ = run_cmd([\"pdftoppm\", \"-png\", \"-r\", str(dpi), str(pdf_path), str(out_prefix)])\n",
    "    results = []\n",
    "    pages = sorted(out_dir.glob(\"page-*.png\"))\n",
    "    for i, p in enumerate(pages):\n",
    "        try:\n",
    "            with Image.open(p) as im:\n",
    "                w, h = im.size\n",
    "        except Exception:\n",
    "            w, h = 0, 0\n",
    "        results.append({\"path\": p, \"page\": i, \"width\": w, \"height\": h})\n",
    "    return results\n",
    "\n",
    "def extract_images_for_doc(pdf_path: Path, out_dir: Path):\n",
    "    if HAVE_PDFIMAGES:\n",
    "        imgs = extract_images_pdfimages(pdf_path, out_dir)\n",
    "    elif HAVE_PDFTOPPM:\n",
    "        imgs = extract_images_pdftoppm(pdf_path, out_dir)\n",
    "    else:\n",
    "        log(f\"No pdfimages/pdftoppm available for {pdf_path}\")\n",
    "        return []\n",
    "    # Sanity: filter tiny images\n",
    "    good = [d for d in imgs if d[\"width\"] >= MIN_W and d[\"height\"] >= MIN_H]\n",
    "    # Cap per doc\n",
    "    return good[:MAX_IMAGES_PER_DOC]\n",
    "\n",
    "# ---------- Embedding backends ----------\n",
    "device = \"cpu\"\n",
    "emb_backend = None\n",
    "emb_dim = None\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# Try open_clip first\n",
    "if torch is not None:\n",
    "    try:\n",
    "        import open_clip\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\", device=device)\n",
    "        model.eval()\n",
    "        emb_backend = \"open_clip_ViT-B-32\"\n",
    "        emb_dim = model.visual.output_dim if hasattr(model, \"visual\") else 512\n",
    "    except Exception:\n",
    "        model = None\n",
    "        preprocess = None\n",
    "        # Try torchvision ViT\n",
    "        try:\n",
    "            import torchvision\n",
    "            from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
    "            model = vit_b_16(weights=weights).to(device)\n",
    "            model.eval()\n",
    "            preprocess = weights.transforms()\n",
    "            emb_backend = \"torchvision_vit_b_16\"\n",
    "            emb_dim = 1000  # classifier logits dim; acceptable as a proxy\n",
    "        except Exception:\n",
    "            model = None\n",
    "            preprocess = None\n",
    "\n",
    "def embed_image(png_path: Path):\n",
    "    if torch is None or model is None or preprocess is None:\n",
    "        return None\n",
    "    try:\n",
    "        with Image.open(png_path).convert(\"RGB\") as img:\n",
    "            inp = preprocess(img)\n",
    "        if hasattr(inp, \"unsqueeze\"):  # torchvision tensor\n",
    "            x = inp.unsqueeze(0).to(device)\n",
    "        else:\n",
    "            import torch as _t\n",
    "            x = _t.tensor(inp).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if \"open_clip\" in (emb_backend or \"\"):\n",
    "                feats = model.encode_image(x)\n",
    "            elif \"torchvision_vit\" in (emb_backend or \"\"):\n",
    "                feats = model(x)\n",
    "            else:\n",
    "                return None\n",
    "            feats = feats.float()\n",
    "            # L2 normalize\n",
    "            feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "            vec = feats.squeeze(0).detach().cpu().numpy()\n",
    "            return vec\n",
    "    except Exception as e:\n",
    "        log(f\"embed fail {png_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Build v2 graph ----------\n",
    "G2 = nx.Graph()\n",
    "\n",
    "# Seed with all v1 nodes/edges so we keep text structure\n",
    "for n in v1[\"nodes\"]:\n",
    "    G2.add_node(n[\"id\"], **{k:v for k,v in n.items() if k!=\"id\"})\n",
    "for e in v1[\"edges\"]:\n",
    "    G2.add_edge(e[\"u\"], e[\"v\"], **{k:v for k,v in e.items() if k not in (\"u\",\"v\")})\n",
    "\n",
    "docs = list(doc_to_pages.items())\n",
    "if MAX_PDFS:\n",
    "    docs = docs[:MAX_PDFS]\n",
    "\n",
    "total_imgs = 0\n",
    "embedded = 0\n",
    "skipped = 0\n",
    "per_doc_stats = []\n",
    "\n",
    "for doc_id, page_list in docs:\n",
    "    pdf_path = doc_to_path.get(doc_id)\n",
    "    if not pdf_path or not Path(pdf_path).exists():\n",
    "        log(f\"missing pdf for {doc_id}\")\n",
    "        continue\n",
    "    docname = doc_id.split(\"::\",1)[1]\n",
    "    doc_stem = docname.rsplit(\".\",1)[0]\n",
    "    out_dir = V2_IMG / doc_stem\n",
    "    imgs = extract_images_for_doc(Path(pdf_path), out_dir)\n",
    "\n",
    "    # map images to v1 page node ids when possible\n",
    "    page_by_idx = {pidx: nid for (nid, pidx) in page_list}\n",
    "    count_doc = 0\n",
    "\n",
    "    for i, d in enumerate(imgs):\n",
    "        page = d[\"page\"]\n",
    "        img_path = d[\"path\"]\n",
    "        w, h = d[\"width\"], d[\"height\"]\n",
    "        total_imgs += 1\n",
    "        count_doc += 1\n",
    "\n",
    "        # Build image node id\n",
    "        img_id = f\"img::{docname}::p{page}::i{i:03d}\"\n",
    "        G2.add_node(img_id, kind=\"image\", page=int(page), width=w, height=h, path=str(img_path))\n",
    "\n",
    "        # Link to page node if known\n",
    "        page_node = page_by_idx.get(int(page))\n",
    "        if page_node:\n",
    "            G2.add_edge(page_node, img_id, kind=\"has_image\")\n",
    "        else:\n",
    "            # link to doc if page unknown\n",
    "            G2.add_edge(doc_id, img_id, kind=\"has_image\")\n",
    "\n",
    "        # Embeddings\n",
    "        if emb_backend is not None:\n",
    "            vec = embed_image(img_path)\n",
    "            if vec is not None:\n",
    "                try:\n",
    "                    import numpy as np\n",
    "                    emb_path = V2_EMB / f\"{doc_stem}_p{page}_i{i:03d}.npy\"\n",
    "                    V2_EMB.mkdir(parents=True, exist_ok=True)\n",
    "                    np.save(emb_path, vec.astype(\"float32\"))\n",
    "                    G2.nodes[img_id][\"embedding\"] = {\"path\": str(emb_path), \"dim\": int(vec.shape[-1]), \"backend\": emb_backend}\n",
    "                    embedded += 1\n",
    "                except Exception:\n",
    "                    # fallback to JSON\n",
    "                    emb_path = V2_EMB / f\"{doc_stem}_p{page}_i{i:03d}.json\"\n",
    "                    V2_EMB.mkdir(parents=True, exist_ok=True)\n",
    "                    emb_path.write_text(json.dumps({\n",
    "                        \"backend\": emb_backend,\n",
    "                        \"vec\": [round(float(x), EMB_FLOATS) for x in (vec.tolist() if hasattr(vec, \"tolist\") else list(vec))],\n",
    "                    }))\n",
    "                    G2.nodes[img_id][\"embedding\"] = {\"path\": str(emb_path), \"dim\": len(G2.nodes[img_id][\"embedding\"].get(\"vec\", [])), \"backend\": emb_backend}\n",
    "                    embedded += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "\n",
    "    per_doc_stats.append({\"doc\": docname, \"images\": count_doc})\n",
    "\n",
    "# Save v2\n",
    "nodes_out = [{\"id\": n, **G2.nodes[n]} for n in G2.nodes]\n",
    "edges_out = [{\"u\": u, \"v\": v, **G2[u][v]} for u, v in G2.edges]\n",
    "V2_GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "V2_GRAPH_JSON.write_text(json.dumps({\"nodes\": nodes_out, \"edges\": edges_out}, indent=2), encoding=\"utf-8\")\n",
    "nx.write_gml(G2, V2_GRAPH_GML)\n",
    "\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"v1_graph\": str(V1_GRAPH_JSON),\n",
    "    \"v2_graph_json\": str(V2_GRAPH_JSON),\n",
    "    \"v2_graph_gml\": str(V2_GRAPH_GML),\n",
    "    \"images_root\": str(V2_IMG),\n",
    "    \"embeddings_root\": str(V2_EMB),\n",
    "    \"stats\": {\n",
    "        \"documents\": len(docs),\n",
    "        \"images_total\": total_imgs,\n",
    "        \"embeddings_built\": embedded,\n",
    "        \"embeddings_skipped\": skipped,\n",
    "    },\n",
    "    \"per_doc\": per_doc_stats,\n",
    "    \"embedding_backend\": emb_backend,\n",
    "}\n",
    "V2_MANIFEST.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Graph-RAG v2 built\")\n",
    "print(\"Docs:\", manifest[\"stats\"][\"documents\"])\n",
    "print(\"Images total:\", total_imgs, \"| embedded:\", embedded, \"| skipped:\", skipped)\n",
    "print(\"Embedding backend:\", emb_backend)\n",
    "print(\"Saved:\")\n",
    "print(\"  -\", V2_GRAPH_JSON)\n",
    "print(\"  -\", V2_GRAPH_GML)\n",
    "print(\"  -\", V2_MANIFEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b62d4a-e5b3-49ef-9830-0c3e6c704919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (core-rag)",
   "language": "python",
   "name": "core-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
