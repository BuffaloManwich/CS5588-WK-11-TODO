{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56582ca3-676e-419a-a002-7c9a7aff09fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HF hub compat shim active\n"
     ]
    }
   ],
   "source": [
    "# HF hub compat shim for older diffusers that still call cached_download\n",
    "import huggingface_hub as _hfh\n",
    "try:\n",
    "    _ = _hfh.cached_download\n",
    "except AttributeError:\n",
    "    from huggingface_hub import hf_hub_download as _hf_dd\n",
    "    _hfh.cached_download = _hf_dd  # make diffusers==0.27.2 happy\n",
    "print(\"✅ HF hub compat shim active\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a315f6d7-030f-4ad1-880e-2e09b6eb5ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVD scaffold ready. Use gen_i2v_svd(image_path, prompt=...) to generate a clip.\n",
      "Note: If packages/models are missing, the function prints a clear hint and returns ok=False.\n"
     ]
    }
   ],
   "source": [
    "# Step V · Cell 1 — Video Generation Scaffold (SVD Image→Video via diffusers)\n",
    "# - No installs here. If libraries/models are missing, prints a clear message and exits gracefully.\n",
    "# - Works great with images extracted by our Graph-RAG v2 (PDF images).\n",
    "# - Outputs: MP4 saved under visual_outputs/, plus a metrics dict.\n",
    "\n",
    "import time, os, math, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "VIS = W11 / \"visual_outputs\"\n",
    "VIS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _has_pkgs():\n",
    "    try:\n",
    "        import torch, diffusers, imageio_ffmpeg  # noqa\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Missing dependency: {e}\"\n",
    "\n",
    "def _load_svd_pipeline(model_id=\"stabilityai/stable-video-diffusion-img2vid-xt\", device_pref=\"cuda\"):\n",
    "    import torch\n",
    "    from diffusers import StableVideoDiffusionPipeline\n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        model_id, torch_dtype=dtype, variant=\"fp16\" if dtype==torch.float16 else None\n",
    "    )\n",
    "    device = device_pref if (device_pref==\"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    # Memory-savers: attention slicing & enable_vae_slicing if available\n",
    "    try: pipe.enable_attention_slicing()\n",
    "    except Exception: pass\n",
    "    try: pipe.enable_vae_slicing()\n",
    "    except Exception: pass\n",
    "    return pipe, device, dtype\n",
    "\n",
    "def gen_i2v_svd(\n",
    "    image_path: str,\n",
    "    prompt: str = \"\",\n",
    "    seed: int = 42,\n",
    "    num_frames: int = 25,\n",
    "    num_inference_steps: int = 20,\n",
    "    motion_bucket_id: int = 127,         # SVD motion strength (0–255-ish; higher = more motion)\n",
    "    noise_aug_strength: float = 0.02,    # small noise to encourage motion\n",
    "    fps: int = 7,                        # output fps\n",
    "    height: int = 576, width: int = 1024 # SVD-XL likes 576p x 1024; can scale down (e.g., 256x256) if VRAM tight\n",
    "):\n",
    "    \"\"\"Generate a short video from a conditioning image using Stable Video Diffusion.\"\"\"\n",
    "    ok, err = _has_pkgs()\n",
    "    if not ok:\n",
    "        msg = f\"[SVD] Dependencies missing. Install torch, diffusers, imageio-ffmpeg. Details: {err}\"\n",
    "        print(msg)\n",
    "        return {\"ok\": False, \"error\": msg}\n",
    "\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import imageio\n",
    "\n",
    "    pipe, device, dtype = _load_svd_pipeline()\n",
    "\n",
    "    # Load/resize image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((width, height), Image.BICUBIC)\n",
    "\n",
    "    # Timers\n",
    "    t0 = time.time()\n",
    "    gen = torch.Generator(device=device)\n",
    "    if \"cuda\" in device:\n",
    "        gen = gen.manual_seed(seed)\n",
    "\n",
    "    # SVD call\n",
    "    result = pipe(\n",
    "        img,\n",
    "        num_frames=num_frames,\n",
    "        decode_chunk_size=8,        # stream decode to save RAM\n",
    "        motion_bucket_id=motion_bucket_id,\n",
    "        noise_aug_strength=noise_aug_strength,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=gen\n",
    "    )\n",
    "    frames = result.frames[0]  # list of PIL images\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Save MP4\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_mp4 = VIS / f\"svd_i2v_{ts}.mp4\"\n",
    "    writer = imageio.get_writer(out_mp4, fps=fps)\n",
    "    for f in frames:\n",
    "        writer.append_data(np.array(f))\n",
    "    writer.close()\n",
    "\n",
    "    # Metrics\n",
    "    wall = t1 - t0\n",
    "    vid_dur = num_frames / float(fps)\n",
    "    rtf = vid_dur / wall if wall > 0 else math.inf\n",
    "\n",
    "    meta = {\n",
    "        \"ok\": True,\n",
    "        \"model\": \"stable-video-diffusion-img2vid-xt\",\n",
    "        \"device\": device,\n",
    "        \"dtype\": str(dtype),\n",
    "        \"prompt\": prompt,\n",
    "        \"seed\": seed,\n",
    "        \"num_frames\": num_frames,\n",
    "        \"fps\": fps,\n",
    "        \"steps\": num_inference_steps,\n",
    "        \"motion_bucket_id\": motion_bucket_id,\n",
    "        \"noise_aug_strength\": noise_aug_strength,\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"out_mp4\": str(out_mp4),\n",
    "        \"wall_time_sec\": round(wall, 3),\n",
    "        \"video_duration_sec\": round(vid_dur, 3),\n",
    "        \"RTF\": round(rtf, 3)\n",
    "    }\n",
    "    # Optional: write sidecar JSON\n",
    "    (out_mp4.with_suffix(\".json\")).write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    print(\"✅ SVD Image→Video done\")\n",
    "    print(\"Output :\", out_mp4)\n",
    "    print(\"Metrics:\", meta)\n",
    "    return meta\n",
    "\n",
    "print(\"✅ SVD scaffold ready. Use gen_i2v_svd(image_path, prompt=...) to generate a clip.\")\n",
    "print(\"Note: If packages/models are missing, the function prints a clear hint and returns ok=False.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c790f6e-4eeb-4aec-8c2d-a94ecc6bc48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] imageio_ffmpeg already installed\n",
      "Query: 'How do I fix a faucet leak?' | text hits kept (after sanity): 5\n",
      "- 1001 do-it-yourself hints & tips  tricks.pdf p13 score=0.305497, snippet='SAFE  AND  SNART  If  you  have  a  flood,  mini-  mize the  damage  with  these...'\n",
      "- 1001 do-it-yourself hints & tips  tricks.pdf p46 score=0.219357, snippet='Seen  from  afar.  Inspect  the  roof  in  spring  and  fall  and  after  severe...'\n",
      "- Safe & Sound _ A Renter-Friendly Guide to Home Repair.pdf p77 score=0.179004, snippet='REPLACING A FAUCET CARTRIDGE OR A STEM tap here to view video MATERIALS Allen ke...'\n",
      "- How to stop Water damage when A Leak.pdf p1 score=0.168118, snippet='frequently.  Install a water leak detector    Water leak detectors square measur...'\n",
      "- EditorsofCoolSp_2022__EssentialHomeSkillsHaPart2.pdf p26 score=0.167639, snippet=' 148 The Essential Home Skills Handbook    How You Do It  1. In the attic, exami...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2e696c53a648129fc0a79e5cc79414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f41349c673a457e9da1066ea9edf2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.57 GiB of which 2.23 GiB is free. Including non-PyTorch memory, this process has 12.79 GiB memory in use. Of the allocated memory 9.89 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 205\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         meta = \u001b[43mgen_i2v_svd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msvd_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_SEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_FPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmotion_bucket_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_MOTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnoise_aug_strength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_NOISE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_SIZE\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSVD_SIZE\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgen_i2v_svd not found; run the SVD scaffold cell first.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mgen_i2v_svd\u001b[39m\u001b[34m(image_path, prompt, seed, num_frames, num_inference_steps, motion_bucket_id, noise_aug_strength, fps, height, width)\u001b[39m\n\u001b[32m     70\u001b[39m     gen = gen.manual_seed(seed)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# SVD call\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m result = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# stream decode to save RAM\u001b[39;49;00m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmotion_bucket_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmotion_bucket_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_aug_strength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_aug_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m frames = result.frames[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# list of PIL images\u001b[39;00m\n\u001b[32m     83\u001b[39m t1 = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py:554\u001b[39m, in \u001b[36mStableVideoDiffusionPipeline.__call__\u001b[39m\u001b[34m(self, image, height, width, num_frames, num_inference_steps, min_guidance_scale, max_guidance_scale, fps, motion_bucket_id, noise_aug_strength, decode_chunk_size, num_videos_per_prompt, generator, latents, output_type, callback_on_step_end, callback_on_step_end_tensor_inputs, return_dict)\u001b[39m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_upcasting:\n\u001b[32m    553\u001b[39m         \u001b[38;5;28mself\u001b[39m.vae.to(dtype=torch.float16)\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m     frames = tensor2vid(frames, \u001b[38;5;28mself\u001b[39m.image_processor, output_type=output_type)\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py:256\u001b[39m, in \u001b[36mStableVideoDiffusionPipeline.decode_latents\u001b[39m\u001b[34m(self, latents, num_frames, decode_chunk_size)\u001b[39m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m accepts_num_frames:\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# we only pass num_frames_in if it's expected\u001b[39;00m\n\u001b[32m    254\u001b[39m         decode_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_frames\u001b[39m\u001b[33m\"\u001b[39m] = num_frames_in\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     frame = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdecode_kwargs\u001b[49m\u001b[43m)\u001b[49m.sample\n\u001b[32m    257\u001b[39m     frames.append(frame)\n\u001b[32m    258\u001b[39m frames = torch.cat(frames, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[39m, in \u001b[36mapply_forward_hook.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_hf_hook\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._hf_hook, \u001b[33m\"\u001b[39m\u001b[33mpre_forward\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m._hf_hook.pre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py:364\u001b[39m, in \u001b[36mAutoencoderKLTemporalDecoder.decode\u001b[39m\u001b[34m(self, z, num_frames, return_dict)\u001b[39m\n\u001b[32m    362\u001b[39m batch_size = z.shape[\u001b[32m0\u001b[39m] // num_frames\n\u001b[32m    363\u001b[39m image_only_indicator = torch.zeros(batch_size, num_frames, dtype=z.dtype, device=z.device)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m decoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m    367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py:147\u001b[39m, in \u001b[36mTemporalDecoder.forward\u001b[39m\u001b[34m(self, sample, image_only_indicator, num_frames)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.up_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         sample = \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[32m    150\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.conv_norm_out(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/models/unets/unet_3d_blocks.py:1867\u001b[39m, in \u001b[36mUpBlockTemporalDecoder.forward\u001b[39m\u001b[34m(self, hidden_states, image_only_indicator)\u001b[39m\n\u001b[32m   1861\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1863\u001b[39m     hidden_states: torch.FloatTensor,\n\u001b[32m   1864\u001b[39m     image_only_indicator: torch.FloatTensor,\n\u001b[32m   1865\u001b[39m ) -> torch.FloatTensor:\n\u001b[32m   1866\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resnets:\n\u001b[32m-> \u001b[39m\u001b[32m1867\u001b[39m         hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1869\u001b[39m \u001b[43m            \u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_only_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1870\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1872\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1873\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/models/resnet.py:713\u001b[39m, in \u001b[36mSpatioTemporalResBlock.forward\u001b[39m\u001b[34m(self, hidden_states, temb, image_only_indicator)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    711\u001b[39m     temb = temb.reshape(batch_size, num_frames, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemporal_res_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.time_mixer(\n\u001b[32m    715\u001b[39m     x_spatial=hidden_states_mix,\n\u001b[32m    716\u001b[39m     x_temporal=hidden_states,\n\u001b[32m    717\u001b[39m     image_only_indicator=image_only_indicator,\n\u001b[32m    718\u001b[39m )\n\u001b[32m    720\u001b[39m hidden_states = hidden_states.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m).reshape(batch_frames, channels, height, width)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/models/resnet.py:618\u001b[39m, in \u001b[36mTemporalResnetBlock.forward\u001b[39m\u001b[34m(self, input_tensor, temb)\u001b[39m\n\u001b[32m    616\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm1(hidden_states)\n\u001b[32m    617\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.nonlinearity(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.time_emb_proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    621\u001b[39m     temb = \u001b[38;5;28mself\u001b[39m.nonlinearity(temb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/conv.py:717\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/conv.py:712\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    702\u001b[39m         F.pad(\n\u001b[32m    703\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    710\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    711\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 15.57 GiB of which 2.23 GiB is free. Including non-PyTorch memory, this process has 12.79 GiB memory in use. Of the allocated memory 9.89 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Step V · Cell 2 (replacement) — Sanity filters + ensure imageio-ffmpeg + Query→Page→Image→SVD\n",
    "# Safe to re-run. Requires the SVD scaffold cell defining gen_i2v_svd(...).\n",
    "\n",
    "import sys, subprocess, importlib, math, re, os, json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Ensure imageio-ffmpeg (import name: imageio_ffmpeg; pip name: imageio-ffmpeg) ----------\n",
    "def ensure_pkg(pip_name: str, import_name: str):\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"[ok] {import_name} already installed\")\n",
    "    except Exception:\n",
    "        print(f\"[pip] installing {pip_name} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pip_name])\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"[ok] installed {pip_name}\")\n",
    "\n",
    "ensure_pkg(\"imageio-ffmpeg\", \"imageio_ffmpeg\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "V2_GRAPH_JSON = W11 / \"artifacts\" / \"graph_v2\" / \"graph\" / \"graph.json\"\n",
    "TFIDF_INDEX   = W11 / \"artifacts\" / \"graph\" / \"index\" / \"tfidf_index.json\"\n",
    "RAW_TEXT_ROOT = W11 / \"artifacts\" / \"graph\" / \"raw_text\"\n",
    "METRICS_CSV   = W11 / \"artifacts\" / \"metrics.csv\"\n",
    "\n",
    "# ---------- Settings ----------\n",
    "USER_QUERY   = \"How do I fix a faucet leak?\"\n",
    "TOP_PAGES    = 5     # search depth (text)\n",
    "IMAGES_PER_P = 1     # images per accepted page\n",
    "# SVD knobs\n",
    "SVD_FRAMES   = 24\n",
    "SVD_FPS      = 8\n",
    "SVD_STEPS    = 18\n",
    "SVD_MOTION   = 127\n",
    "SVD_NOISE    = 0.02\n",
    "SVD_SEED     = 42\n",
    "SVD_SIZE     = (576, 1024)  # (H, W)\n",
    "\n",
    "# ---------- Sanity filters ----------\n",
    "WORD = re.compile(r\"[a-zA-Z0-9]+(?:'[a-z0-9]+)?\")\n",
    "STOP = set(\"a an and are as at be by for from has have in is it its of on or that the to with your you we he she they them their our\".split())\n",
    "\n",
    "def sanity_text(snippet: str, min_chars=100, max_nonword_ratio=0.35, min_letter_ratio=0.25):\n",
    "    s = snippet or \"\"\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    if len(s.strip()) < min_chars:\n",
    "        return False, \"too_short\"\n",
    "    total = len(s)\n",
    "    letters = sum(ch.isalpha() for ch in s)\n",
    "    allowed = set(\" .,:;!?-()'\\\"/%[]{}<>|_=+\\\\\\t\")\n",
    "    nonword = sum(not (ch.isalnum() or ch.isspace() or ch in allowed) for ch in s)\n",
    "    if total > 0 and (nonword / total) > max_nonword_ratio:\n",
    "        return False, \"gibberish_ratio\"\n",
    "    if total > 0 and (letters / total) < min_letter_ratio:\n",
    "        return False, \"low_letter_density\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def sanity_image(path: Path, min_w=128, min_h=128, max_aspect=4.0, min_entropy=3.0, min_var=40.0):\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "        with Image.open(path) as im:\n",
    "            w, h = im.size\n",
    "            if w < min_w or h < min_h:\n",
    "                return False, \"too_small\"\n",
    "            ar = max(w/h, h/w)\n",
    "            if ar > max_aspect:\n",
    "                return False, \"extreme_aspect\"\n",
    "            g = im.convert(\"L\")\n",
    "            arr = np.asarray(g)\n",
    "            # entropy\n",
    "            hist = np.bincount(arr.flatten(), minlength=256).astype(\"float32\")\n",
    "            p = hist / (hist.sum() + 1e-8)\n",
    "            ent = float(-(p[p>0] * np.log2(p[p>0])).sum())\n",
    "            # variance\n",
    "            var = float(arr.var())\n",
    "            if ent < min_entropy or var < min_var:\n",
    "                return False, f\"low_info(ent={ent:.2f},var={var:.1f})\"\n",
    "            return True, \"ok\"\n",
    "    except Exception as e:\n",
    "        return False, f\"img_error:{e}\"\n",
    "\n",
    "# ---------- Minimal TF-IDF search (load existing index) ----------\n",
    "def load_index(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing TF-IDF index at {path}\")\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "tf = load_index(TFIDF_INDEX)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [w.lower() for w in WORD.findall(text) if w.lower() not in STOP and len(w) > 1]\n",
    "\n",
    "def search_pages_local(query: str, top_k: int = 5):\n",
    "    q_toks = tokenize(query)\n",
    "    if not q_toks:\n",
    "        return []\n",
    "    q_tf = Counter(q_toks)\n",
    "    # query vec\n",
    "    q_vec = {}\n",
    "    for term, c in q_tf.items():\n",
    "        idf = tf[\"idf\"].get(term, 0.0)\n",
    "        w = (c / max(1, len(q_toks))) * idf\n",
    "        if w > 0: q_vec[term] = w\n",
    "    q_norm = math.sqrt(sum(v*v for v in q_vec.values())) or 1.0\n",
    "\n",
    "    scores = []\n",
    "    N = tf[\"N\"]\n",
    "    for i in range(N):\n",
    "        tf_top = tf[\"tf_top\"][i]\n",
    "        dot = 0.0\n",
    "        for t, qw in q_vec.items():\n",
    "            if t in tf_top:\n",
    "                dw = (tf_top[t] / 200.0) * tf[\"idf\"].get(t, 0.0)\n",
    "                dot += qw * dw\n",
    "        denom = (q_norm * (tf[\"norms\"][i] or 1.0))\n",
    "        s = dot / denom if denom else 0.0\n",
    "        if s > 0:\n",
    "            scores.append((s, i))\n",
    "    scores.sort(reverse=True)\n",
    "    hits = scores[:top_k]\n",
    "\n",
    "    # add snippet + sanity\n",
    "    kept = []\n",
    "    for score, i in hits:\n",
    "        meta = tf[\"docs\"][i]  # {doc, stem, page_idx}\n",
    "        raw_txt = RAW_TEXT_ROOT / meta[\"stem\"] / f\"page_{meta['page_idx']:04d}.txt\"\n",
    "        snippet = raw_txt.read_text(encoding=\"utf-8\", errors=\"ignore\")[:800] if raw_txt.exists() else \"\"\n",
    "        ok, why = sanity_text(snippet)\n",
    "        if not ok:\n",
    "            # print(f\"[drop text] {meta['doc']} p{meta['page_idx']} -> {why}\")\n",
    "            continue\n",
    "        kept.append({\n",
    "            \"score\": round(float(score), 6),\n",
    "            \"doc\": meta[\"doc\"],\n",
    "            \"stem\": meta[\"stem\"],\n",
    "            \"page\": meta[\"page_idx\"],\n",
    "            \"snippet\": snippet.replace(\"\\n\", \" \")[:240],\n",
    "        })\n",
    "    return kept\n",
    "\n",
    "# ---------- Load graph_v2 and page→image mapping ----------\n",
    "if not V2_GRAPH_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Missing graph_v2 JSON at {V2_GRAPH_JSON}\")\n",
    "g2 = json.loads(V2_GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "nodes = {n[\"id\"]: n for n in g2[\"nodes\"]}\n",
    "adj   = defaultdict(list)\n",
    "for e in g2[\"edges\"]:\n",
    "    adj[e[\"u\"]].append((e[\"v\"], e))\n",
    "    adj[e[\"v\"]].append((e[\"u\"], e))\n",
    "\n",
    "def page_node_id(doc: str, page_idx: int):\n",
    "    return f\"doc::{doc}::p{page_idx}\"\n",
    "\n",
    "def images_for_page_filtered(doc: str, page_idx: int, k=1):\n",
    "    pid = page_node_id(doc, page_idx)\n",
    "    if pid not in nodes:\n",
    "        return []\n",
    "    imgs = []\n",
    "    for nbr, eprops in adj[pid]:\n",
    "        nd = nodes.get(nbr, {})\n",
    "        if nd.get(\"kind\") == \"image\":\n",
    "            p = Path(nd.get(\"path\", \"\"))\n",
    "            w, h = nd.get(\"width\", 0), nd.get(\"height\", 0)\n",
    "            if not p.exists():\n",
    "                continue\n",
    "            ok, why = sanity_image(p, min_w=128, min_h=128, max_aspect=4.0, min_entropy=3.0, min_var=40.0)\n",
    "            if ok:\n",
    "                imgs.append({\"id\": nbr, \"path\": str(p), \"w\": w, \"h\": h, \"page\": page_idx, \"doc\": doc})\n",
    "            # else: print(f\"[drop img] {p} -> {why}\")\n",
    "    # prefer largest area\n",
    "    imgs.sort(key=lambda x: x[\"w\"]*x[\"h\"], reverse=True)\n",
    "    return imgs[:k]\n",
    "\n",
    "# ---------- Run pipeline ----------\n",
    "hits = search_pages_local(USER_QUERY, top_k=TOP_PAGES)\n",
    "print(f\"Query: {USER_QUERY!r} | text hits kept (after sanity): {len(hits)}\")\n",
    "for h in hits:\n",
    "    print(f\"- {h['doc']} p{h['page']} score={h['score']}, snippet='{h['snippet'][:80]}...'\")\n",
    "\n",
    "# Gentle SVD prompt tailored to home-repair context\n",
    "svd_prompt = (\n",
    "    \"Short instructional clip of a bathroom faucet, focusing on handle and spout; \"\n",
    "    \"subtle motion; neutral lighting; show the area where drips occur and the place to tighten/adjust.\"\n",
    ")\n",
    "\n",
    "# CSV header\n",
    "if not METRICS_CSV.exists():\n",
    "    METRICS_CSV.write_text(\n",
    "        \"timestamp,pipeline,domain,query,source_doc,source_page,image_path,model,frames,fps,steps,motion,noise,height,width,wall_s,video_s,RTF,out_mp4\\n\"\n",
    "    )\n",
    "\n",
    "# Generate (requires gen_i2v_svd defined earlier)\n",
    "made = 0\n",
    "outputs = []\n",
    "for h in hits:\n",
    "    imgs = images_for_page_filtered(h[\"doc\"], h[\"page\"], k=IMAGES_PER_P)\n",
    "    if not imgs:\n",
    "        continue\n",
    "    for img in imgs:\n",
    "        try:\n",
    "            meta = gen_i2v_svd(\n",
    "                img[\"path\"], prompt=svd_prompt, seed=SVD_SEED,\n",
    "                num_frames=SVD_FRAMES, fps=SVD_FPS,\n",
    "                num_inference_steps=SVD_STEPS,\n",
    "                motion_bucket_id=SVD_MOTION,\n",
    "                noise_aug_strength=SVD_NOISE,\n",
    "                height=SVD_SIZE[0], width=SVD_SIZE[1]\n",
    "            )\n",
    "        except NameError:\n",
    "            print(\"gen_i2v_svd not found; run the SVD scaffold cell first.\")\n",
    "            meta = {\"ok\": False, \"error\": \"missing_svd_scaffold\"}\n",
    "\n",
    "        if meta.get(\"ok\"):\n",
    "            made += 1\n",
    "            outputs.append(meta[\"out_mp4\"])\n",
    "            line = \",\".join([\n",
    "                datetime.now().isoformat(),\n",
    "                \"Image2Video-SVD\",\n",
    "                \"home_repair\",\n",
    "                '\"' + USER_QUERY.replace('\"', \"'\") + '\"',\n",
    "                '\"' + h[\"doc\"] + '\"',\n",
    "                str(h[\"page\"]),\n",
    "                '\"' + img[\"path\"] + '\"',\n",
    "                meta.get(\"model\",\"\"),\n",
    "                str(meta.get(\"num_frames\", SVD_FRAMES)),\n",
    "                str(meta.get(\"fps\", SVD_FPS)),\n",
    "                str(meta.get(\"steps\", SVD_STEPS)),\n",
    "                str(meta.get(\"motion_bucket_id\", SVD_MOTION)),\n",
    "                str(meta.get(\"noise_aug_strength\", SVD_NOISE)),\n",
    "                str(meta.get(\"height\", SVD_SIZE[0])),\n",
    "                str(meta.get(\"width\", SVD_SIZE[1])),\n",
    "                str(meta.get(\"wall_time_sec\",\"\")),\n",
    "                str(meta.get(\"video_duration_sec\",\"\")),\n",
    "                str(meta.get(\"RTF\",\"\")),\n",
    "                '\"' + meta.get(\"out_mp4\",\"\") + '\"',\n",
    "            ]) + \"\\n\"\n",
    "            with METRICS_CSV.open(\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(line)\n",
    "\n",
    "print(f\"\\n✅ Done. Clips generated: {made}\")\n",
    "if outputs:\n",
    "    print(\"Outputs:\")\n",
    "    for o in outputs:\n",
    "        print(\" -\", o)\n",
    "print(\"Metrics CSV ->\", METRICS_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef376a1-8af4-4794-ad41-783bda88f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory-efficient SVD cell loaded. Use gen_i2v_svd_memsafe(image_path, prompt=...)\n"
     ]
    }
   ],
   "source": [
    "# Step V · Cell 2 — Query→Page→Image(s)→SVD, with light sanity filters + metrics logging\n",
    "# Inputs:\n",
    "#   - graph_v2 JSON with image nodes and has_image edges\n",
    "#   - TF-IDF page index (built earlier)\n",
    "#   - SVD generator function: gen_i2v_svd(image_path, prompt=..., ...)\n",
    "#\n",
    "# Outputs:\n",
    "#   - 1..K MP4 files in visual_outputs/\n",
    "#   - artifacts/metrics.csv appended with run metrics\n",
    "# Step V · Cell 2 (replacement) — Sanity filters + ensure imageio-ffmpeg + Query→Page→Image→SVD\n",
    "# Safe to re-run. Requires the SVD scaffold cell defining gen_i2v_svd(...).\n",
    "# Week11-HO-2 · Cell 2 — Memory-efficient SVD loader + generator (auto fallback on OOM)\n",
    "# Requirements: diffusers==0.27.2, accelerate>=0.30,<0.35, torch 2.9.0 (CUDA 12.8),\n",
    "#               imageio, imageio-ffmpeg, pillow (already in your env).\n",
    "# Notes:\n",
    "#  - Prefers bf16 on Ada (RTX 40xx). Falls back to fp16 then fp32.\n",
    "#  - Uses attention/vae slicing + tiling; tries CPU offload to use system RAM.\n",
    "#  - Auto backoff on OOM: lowers resolution → frames → decode_chunk_size → offload mode → dtype.\n",
    "\n",
    "import os, gc, time, math, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "\n",
    "# ---- hard-disable fast-attention paths (keep this belt-and-suspenders) ----\n",
    "os.environ[\"TRANSFORMERS_NO_FLASH_ATTENTION\"] = \"1\"\n",
    "os.environ[\"HF_USE_FLASH_ATTENTION_2\"] = \"0\"\n",
    "os.environ[\"USE_FLASH_ATTENTION\"] = \"0\"\n",
    "\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "VIS = W11 / \"visual_outputs\"\n",
    "VIS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _supports_bf16():\n",
    "    try:\n",
    "        return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _oom(e: Exception) -> bool:\n",
    "    s = str(e).lower()\n",
    "    return isinstance(e, torch.cuda.OutOfMemoryError) or \"out of memory\" in s or \"cuda oom\" in s\n",
    "\n",
    "def _torch_gc():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "def load_svd_memsafe(\n",
    "    model_id: str = \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "    device_pref: str = \"cuda\",\n",
    "    dtype_pref: str = \"bf16\",     # \"bf16\"|\"fp16\"|\"fp32\"\n",
    "    offload_pref: str = \"sequential\"  # \"sequential\"|\"model\"|None\n",
    "):\n",
    "    # dtype resolution\n",
    "    if dtype_pref == \"bf16\" and not _supports_bf16():\n",
    "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    elif dtype_pref == \"fp16\" and torch.cuda.is_available():\n",
    "        dtype = torch.float16\n",
    "    elif dtype_pref == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    elif dtype_pref == \"fp32\":\n",
    "        dtype = torch.float32\n",
    "    else:\n",
    "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    # device selection\n",
    "    device = device_pref if (device_pref == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "\n",
    "    # perf toggles (safe)\n",
    "    try: torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    except Exception: pass\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception: pass\n",
    "\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=dtype,\n",
    "        # NOTE: don't pass variant=\"fp16\" so we freely cast to bf16 if needed.\n",
    "    )\n",
    "\n",
    "    # move/prepare\n",
    "    pipe.to(device)\n",
    "\n",
    "    # memory savers\n",
    "    try: pipe.enable_attention_slicing(\"max\")\n",
    "    except Exception: pass\n",
    "    try: pipe.enable_vae_slicing()\n",
    "    except Exception: pass\n",
    "    try: pipe.enable_vae_tiling()\n",
    "    except Exception: pass\n",
    "\n",
    "    # CPU offload (requires accelerate)\n",
    "    if device == \"cuda\":\n",
    "        try:\n",
    "            if offload_pref == \"sequential\":\n",
    "                pipe.enable_sequential_cpu_offload()\n",
    "            elif offload_pref == \"model\":\n",
    "                pipe.enable_model_cpu_offload()\n",
    "        except Exception:\n",
    "            # if accelerate not cooperating, continue without offload\n",
    "            pass\n",
    "\n",
    "    # quieter progress bar\n",
    "    try: pipe.set_progress_bar_config(disable=True)\n",
    "    except Exception: pass\n",
    "\n",
    "    return pipe, device, str(dtype)\n",
    "\n",
    "def gen_i2v_svd_memsafe(\n",
    "    image_path: str,\n",
    "    prompt: str = \"\",\n",
    "    seed: int = 42,\n",
    "    # initial targets (will be reduced on OOM)\n",
    "    height: int = 576, width: int = 1024,\n",
    "    num_frames: int = 24,\n",
    "    fps: int = 8,\n",
    "    num_inference_steps: int = 18,\n",
    "    motion_bucket_id: int = 127,\n",
    "    noise_aug_strength: float = 0.02,\n",
    "    decode_chunk_size: int = 8,\n",
    "    # loader prefs\n",
    "    dtype_pref: str = \"bf16\",\n",
    "    offload_pref: str = \"sequential\",\n",
    "    device_pref: str = \"cuda\",\n",
    "    model_id: str = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: meta dict with ok, out_mp4, and the final knobs used. Writes sidecar JSON.\n",
    "    Auto-backoff order on OOM:\n",
    "      (a) lower res → (b) fewer frames → (c) smaller decode_chunk → (d) stronger offload → (e) relax dtype.\n",
    "    \"\"\"\n",
    "\n",
    "    # Backoff ladders\n",
    "    size_candidates = [(height, width), (448, 768), (384, 672), (320, 576), (256, 448), (192, 336)]\n",
    "    frame_candidates = [num_frames, 20, 16, 12, 8]\n",
    "    chunk_candidates = [decode_chunk_size, 6, 4, 2, 1]\n",
    "    offload_candidates = [offload_pref, \"sequential\", \"model\", None]\n",
    "    dtype_candidates = [dtype_pref, \"bf16\", \"fp16\", \"fp32\"]\n",
    "\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    tried = []\n",
    "    t_start = time.time()\n",
    "    last_err = None\n",
    "    out_meta = None\n",
    "\n",
    "    for dty in dtype_candidates:\n",
    "        for off in offload_candidates:\n",
    "            # Load/reload the pipeline per (dtype, offload) combo for clean memory\n",
    "            _torch_gc()\n",
    "            pipe, device, dtype_str = load_svd_memsafe(\n",
    "                model_id=model_id,\n",
    "                device_pref=device_pref,\n",
    "                dtype_pref=dty,\n",
    "                offload_pref=off\n",
    "            )\n",
    "            # enable chunked decoding by default; we'll vary the chunk size in the loop\n",
    "            ok_this_combo = False\n",
    "\n",
    "            for (h, w) in size_candidates:\n",
    "                # resize image up-front to target res\n",
    "                img_hw = img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "                for nf in frame_candidates:\n",
    "                    for chunk in chunk_candidates:\n",
    "                        tried.append((dty, off, h, w, nf, chunk))\n",
    "                        _torch_gc()\n",
    "                        try:\n",
    "                            gen = torch.Generator(device=device)\n",
    "                            if device == \"cuda\":\n",
    "                                gen = gen.manual_seed(seed)\n",
    "\n",
    "                            t0 = time.time()\n",
    "                            with torch.autocast(device_type=(\"cuda\" if device==\"cuda\" else \"cpu\"),\n",
    "                                                dtype=torch.bfloat16 if (dty==\"bf16\" and _supports_bf16()) else\n",
    "                                                      (torch.float16 if dty==\"fp16\" and torch.cuda.is_available() else torch.float32)):\n",
    "                                out = pipe(\n",
    "                                    img_hw,\n",
    "                                    num_frames=nf,\n",
    "                                    decode_chunk_size=chunk,\n",
    "                                    motion_bucket_id=motion_bucket_id,\n",
    "                                    noise_aug_strength=noise_aug_strength,\n",
    "                                    num_inference_steps=num_inference_steps,\n",
    "                                    generator=gen\n",
    "                                )\n",
    "                            frames = out.frames[0]  # list of PIL images\n",
    "                            t1 = time.time()\n",
    "\n",
    "                            # Write MP4\n",
    "                            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                            out_mp4 = VIS / f\"svd_i2v_mem_{ts}.mp4\"\n",
    "                            writer = imageio.get_writer(out_mp4, fps=fps)\n",
    "                            for f in frames:\n",
    "                                writer.append_data(np.array(f))\n",
    "                            writer.close()\n",
    "\n",
    "                            # Metrics\n",
    "                            wall = t1 - t0\n",
    "                            vid_dur = nf / float(fps)\n",
    "                            rtf = vid_dur / wall if wall > 0 else math.inf\n",
    "\n",
    "                            out_meta = {\n",
    "                                \"ok\": True,\n",
    "                                \"model\": model_id,\n",
    "                                \"device\": device,\n",
    "                                \"dtype\": dtype_str,\n",
    "                                \"offload\": off,\n",
    "                                \"seed\": seed,\n",
    "                                \"frames\": nf,\n",
    "                                \"fps\": fps,\n",
    "                                \"steps\": num_inference_steps,\n",
    "                                \"motion_bucket_id\": motion_bucket_id,\n",
    "                                \"noise_aug_strength\": noise_aug_strength,\n",
    "                                \"height\": h,\n",
    "                                \"width\": w,\n",
    "                                \"decode_chunk_size\": chunk,\n",
    "                                \"out_mp4\": str(out_mp4),\n",
    "                                \"wall_time_sec\": round(wall, 3),\n",
    "                                \"video_duration_sec\": round(vid_dur, 3),\n",
    "                                \"RTF\": round(rtf, 3),\n",
    "                                \"tried_order\": tried[-6:],  # last few attempts for debug\n",
    "                            }\n",
    "                            (out_mp4.with_suffix(\".json\")).write_text(json.dumps(out_meta, indent=2), encoding=\"utf-8\")\n",
    "                            print(\"✅ SVD (memsafe) done:\", out_mp4)\n",
    "                            print(\"    used:\", {\"dtype\": dtype_str, \"offload\": off, \"H\": h, \"W\": w, \"frames\": nf, \"chunk\": chunk})\n",
    "                            ok_this_combo = True\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            last_err = e\n",
    "                            if _oom(e):\n",
    "                                print(f\"[OOM fallback] dtype={dty}, offload={off}, {h}x{w}, frames={nf}, chunk={chunk} -> reducing...\")\n",
    "                            else:\n",
    "                                print(f\"[error] {type(e).__name__}: {e}\")\n",
    "                            _torch_gc()\n",
    "                            continue\n",
    "                    if ok_this_combo: break\n",
    "                if ok_this_combo: break\n",
    "            if ok_this_combo: break\n",
    "\n",
    "    if out_meta and out_meta.get(\"ok\"):\n",
    "        return out_meta\n",
    "\n",
    "    # If we get here, all fallbacks failed\n",
    "    print(\"❌ All memory fallbacks exhausted.\")\n",
    "    if last_err: print(\"Last error:\", type(last_err).__name__, str(last_err))\n",
    "    return {\"ok\": False, \"error\": str(last_err) if last_err else \"unknown\", \"tried\": tried, \"model\": model_id}\n",
    "\n",
    "# ---- quick usage hint (comment out when integrating with your pipeline) ----\n",
    "print(\"✅ Memory-efficient SVD cell loaded. Use gen_i2v_svd_memsafe(image_path, prompt=...)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd29ca3-3e0b-434a-8cec-6dba1baf3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'faucet spout' | candidate pages: 6\n",
      "Chosen image: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-013-037.png (from 1001 do-it-yourself hints & tips  tricks.pdf p13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17637984e1a84fba81b2a0ce6edcc564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=2 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_204018.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bb042420f24bbd968127573c5960c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=2 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_204158.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8162b004f5a476e893a6b862016a5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=8, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=8, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=8, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=8, chunk=2 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_204440.mp4\n",
      "    used: {'dtype': 'torch.float16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 8, 'chunk': 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64382bda6904428180bba04393860cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 27.50 MiB is free. Including non-PyTorch memory, this process has 15.00 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 162.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChosen image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchosen[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchosen_hit[\u001b[33m'\u001b[39m\u001b[33mdoc\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m p\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchosen_hit[\u001b[33m'\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    163\u001b[39m svd_prompt = (\n\u001b[32m    164\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mShort instructional clip showing the faucet spout area; subtle motion; neutral lighting; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    165\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfocus on where drips occur and what to inspect.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m meta = \u001b[43mgen_i2v_svd_memsafe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchosen\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msvd_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# leave defaults; mem-safe function will auto-backoff if OOM\u001b[39;49;00m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbf16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msequential\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta.get(\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# append metrics\u001b[39;00m\n\u001b[32m    175\u001b[39m     line = \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join([\n\u001b[32m    176\u001b[39m         datetime.now().isoformat(),\n\u001b[32m    177\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImage2Video-SVD-memsafe\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    195\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m + meta.get(\u001b[33m\"\u001b[39m\u001b[33mout_mp4\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    196\u001b[39m     ]) + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mgen_i2v_svd_memsafe\u001b[39m\u001b[34m(image_path, prompt, seed, height, width, num_frames, fps, num_inference_steps, motion_bucket_id, noise_aug_strength, decode_chunk_size, dtype_pref, offload_pref, device_pref, model_id)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m off \u001b[38;5;129;01min\u001b[39;00m offload_candidates:\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# Load/reload the pipeline per (dtype, offload) combo for clean memory\u001b[39;00m\n\u001b[32m    159\u001b[39m     _torch_gc()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     pipe, device, dtype_str = \u001b[43mload_svd_memsafe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43moff\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# enable chunked decoding by default; we'll vary the chunk size in the loop\u001b[39;00m\n\u001b[32m    167\u001b[39m     ok_this_combo = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mload_svd_memsafe\u001b[39m\u001b[34m(model_id, device_pref, dtype_pref, offload_pref)\u001b[39m\n\u001b[32m     83\u001b[39m pipe = StableVideoDiffusionPipeline.from_pretrained(\n\u001b[32m     84\u001b[39m     model_id,\n\u001b[32m     85\u001b[39m     torch_dtype=dtype,\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# NOTE: don't pass variant=\"fp16\" so we freely cast to bf16 if needed.\u001b[39;00m\n\u001b[32m     87\u001b[39m )\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# move/prepare\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# memory savers\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m: pipe.enable_attention_slicing(\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:418\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m     logger.warning(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been loaded in 8bit and moving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m     )\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    421\u001b[39m     module.dtype == torch.float16\n\u001b[32m    422\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    423\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    424\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    425\u001b[39m ):\n\u001b[32m    426\u001b[39m     logger.warning(\n\u001b[32m    427\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (5 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 27.50 MiB is free. Including non-PyTorch memory, this process has 15.00 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 162.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Week11-HO-2 · Cell — Query \"faucet spout\" → choose one image → mem-safe SVD video + metrics\n",
    "# Assumes:\n",
    "#   - TF-IDF index exists at artifacts/graph/index/tfidf_index.json (with early sanity).\n",
    "#   - Graph v2 exists at artifacts/graph_v2/graph/graph.json (with image nodes).\n",
    "#   - gen_i2v_svd_memsafe(...) is already defined in a previous cell.\n",
    "# Outputs:\n",
    "#   - One MP4 in visual_outputs/\n",
    "#   - Append one line to artifacts/metrics.csv\n",
    "\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Paths ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "TFIDF_INDEX   = W11 / \"artifacts\" / \"graph\" / \"index\" / \"tfidf_index.json\"\n",
    "RAW_TEXT_ROOT = W11 / \"artifacts\" / \"graph\" / \"raw_text\"\n",
    "V2_GRAPH_JSON = W11 / \"artifacts\" / \"graph_v2\" / \"graph\" / \"graph.json\"\n",
    "METRICS_CSV   = W11 / \"artifacts\" / \"metrics.csv\"\n",
    "\n",
    "# ---------- Query ----------\n",
    "USER_QUERY = \"faucet spout\"     # focused image search phrase\n",
    "TOP_PAGES  = 6                  # search depth\n",
    "IMAGES_PER_P_CANDIDATES = 4     # how many images to consider per page before picking one\n",
    "\n",
    "# ---------- Small search helpers (standalone) ----------\n",
    "WORD = re.compile(r\"[a-zA-Z0-9]+(?:'[a-z0-9]+)?\")\n",
    "STOP = set(\"a an and are as at be by for from has have in is it its of on or that the to with your you we he she they them their our\".split())\n",
    "\n",
    "def tokenize(text):\n",
    "    return [w.lower() for w in WORD.findall(text) if w.lower() not in STOP and len(w) > 1]\n",
    "\n",
    "def load_index(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing TF-IDF index at {path}\")\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "tf = load_index(TFIDF_INDEX)\n",
    "\n",
    "def search_pages_local(query: str, top_k: int = 5):\n",
    "    q_toks = tokenize(query)\n",
    "    if not q_toks:\n",
    "        return []\n",
    "    q_tf = Counter(q_toks)\n",
    "    q_vec = {}\n",
    "    for term, c in q_tf.items():\n",
    "        idf = tf[\"idf\"].get(term, 0.0)\n",
    "        w = (c / max(1, len(q_toks))) * idf\n",
    "        if w > 0: q_vec[term] = w\n",
    "    q_norm = math.sqrt(sum(v*v for v in q_vec.values())) or 1.0\n",
    "\n",
    "    scores = []\n",
    "    N = tf[\"N\"]\n",
    "    for i in range(N):\n",
    "        tf_top = tf[\"tf_top\"][i]\n",
    "        dot = 0.0\n",
    "        for t, qw in q_vec.items():\n",
    "            if t in tf_top:\n",
    "                dw = (tf_top[t] / 200.0) * tf[\"idf\"].get(t, 0.0)\n",
    "                dot += qw * dw\n",
    "        denom = (q_norm * (tf[\"norms\"][i] or 1.0))\n",
    "        s = dot / denom if denom else 0.0\n",
    "        if s > 0:\n",
    "            scores.append((s, i))\n",
    "    scores.sort(reverse=True)\n",
    "    hits = scores[:top_k]\n",
    "\n",
    "    results = []\n",
    "    for score, i in hits:\n",
    "        meta = tf[\"docs\"][i]  # {doc, stem, page_idx}\n",
    "        # read a snippet just for display (index is already sanity-filtered)\n",
    "        raw_txt = RAW_TEXT_ROOT / meta[\"stem\"] / f\"page_{meta['page_idx']:04d}.txt\"\n",
    "        snippet = raw_txt.read_text(encoding=\"utf-8\", errors=\"ignore\")[:360] if raw_txt.exists() else \"\"\n",
    "        results.append({\n",
    "            \"score\": round(float(score), 6),\n",
    "            \"doc\": meta[\"doc\"],\n",
    "            \"stem\": meta[\"stem\"],\n",
    "            \"page\": meta[\"page_idx\"],\n",
    "            \"snippet\": snippet.replace(\"\\n\", \" \"),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ---------- Load graph v2 (page → images) ----------\n",
    "if not V2_GRAPH_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Missing graph_v2 JSON at {V2_GRAPH_JSON}\")\n",
    "g2 = json.loads(V2_GRAPH_JSON.read_text(encoding=\"utf-8\"))\n",
    "nodes = {n[\"id\"]: n for n in g2[\"nodes\"]}\n",
    "adj   = defaultdict(list)\n",
    "for e in g2[\"edges\"]:\n",
    "    adj[e[\"u\"]].append((e[\"v\"], e))\n",
    "    adj[e[\"v\"]].append((e[\"u\"], e))\n",
    "\n",
    "def page_node_id(doc: str, page_idx: int):\n",
    "    return f\"doc::{doc}::p{page_idx}\"\n",
    "\n",
    "def image_info_ok(p: Path, min_w=128, min_h=128, max_aspect=4.0, min_entropy=2.8, min_var=30.0):\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            w, h = im.size\n",
    "            if w < min_w or h < min_h:\n",
    "                return False\n",
    "            ar = max(w/h, h/w)\n",
    "            if ar > max_aspect:\n",
    "                return False\n",
    "            g = im.convert(\"L\")\n",
    "            arr = np.asarray(g)\n",
    "            # entropy + variance (quick info content check)\n",
    "            hist = np.bincount(arr.flatten(), minlength=256).astype(\"float32\")\n",
    "            p = hist / (hist.sum() + 1e-8)\n",
    "            ent = float(-(p[p>0] * np.log2(p[p>0])).sum())\n",
    "            var = float(arr.var())\n",
    "            return ent >= min_entropy and var >= min_var\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def pick_best_image_for_page(doc: str, page_idx: int, k_cand=IMAGES_PER_P_CANDIDATES):\n",
    "    pid = page_node_id(doc, page_idx)\n",
    "    if pid not in nodes:\n",
    "        return None\n",
    "    cand = []\n",
    "    for nbr, _ in adj[pid]:\n",
    "        nd = nodes.get(nbr, {})\n",
    "        if nd.get(\"kind\") == \"image\":\n",
    "            p = Path(nd.get(\"path\", \"\"))\n",
    "            w, h = nd.get(\"width\", 0), nd.get(\"height\", 0)\n",
    "            if p.exists() and image_info_ok(p):\n",
    "                cand.append({\"path\": str(p), \"w\": w, \"h\": h})\n",
    "    cand.sort(key=lambda x: x[\"w\"]*x[\"h\"], reverse=True)\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "# ---------- Ensure metrics CSV has a header ----------\n",
    "if not METRICS_CSV.exists():\n",
    "    METRICS_CSV.write_text(\n",
    "        \"timestamp,pipeline,domain,query,source_doc,source_page,image_path,model,frames,fps,steps,motion,noise,height,width,chunk,wall_s,video_s,RTF,out_mp4\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- Guard: mem-safe generator must exist ----------\n",
    "try:\n",
    "    gen_i2v_svd_memsafe\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"gen_i2v_svd_memsafe(...) not found. Run the Memory-efficient SVD cell first.\") from e\n",
    "\n",
    "# ---------- Run: search → choose image → generate one clip ----------\n",
    "hits = search_pages_local(USER_QUERY, top_k=TOP_PAGES)\n",
    "print(f\"Query: {USER_QUERY!r} | candidate pages: {len(hits)}\")\n",
    "chosen = None\n",
    "chosen_hit = None\n",
    "for h in hits:\n",
    "    img = pick_best_image_for_page(h[\"doc\"], h[\"page\"])\n",
    "    if img:\n",
    "        chosen = img\n",
    "        chosen_hit = h\n",
    "        break\n",
    "\n",
    "if not chosen:\n",
    "    print(\"No suitable images found for the query. Try broadening the phrase or increasing TOP_PAGES.\")\n",
    "else:\n",
    "    print(f\"Chosen image: {chosen['path']} (from {chosen_hit['doc']} p{chosen_hit['page']})\")\n",
    "    svd_prompt = (\n",
    "        \"Short instructional clip showing the faucet spout area; subtle motion; neutral lighting; \"\n",
    "        \"focus on where drips occur and what to inspect.\"\n",
    "    )\n",
    "    meta = gen_i2v_svd_memsafe(\n",
    "        chosen[\"path\"],\n",
    "        prompt=svd_prompt,\n",
    "        # leave defaults; mem-safe function will auto-backoff if OOM\n",
    "        dtype_pref=\"bf16\", offload_pref=\"sequential\", device_pref=\"cuda\"\n",
    "    )\n",
    "    if meta.get(\"ok\"):\n",
    "        # append metrics\n",
    "        line = \",\".join([\n",
    "            datetime.now().isoformat(),\n",
    "            \"Image2Video-SVD-memsafe\",\n",
    "            \"home_repair\",\n",
    "            '\"' + USER_QUERY.replace('\"', \"'\") + '\"',\n",
    "            '\"' + chosen_hit[\"doc\"] + '\"',\n",
    "            str(chosen_hit[\"page\"]),\n",
    "            '\"' + chosen[\"path\"] + '\"',\n",
    "            meta.get(\"model\",\"\"),\n",
    "            str(meta.get(\"frames\",\"\")),\n",
    "            str(meta.get(\"fps\",\"\")),\n",
    "            str(meta.get(\"steps\",\"\")),\n",
    "            str(meta.get(\"motion_bucket_id\",\"\")),\n",
    "            str(meta.get(\"noise_aug_strength\",\"\")),\n",
    "            str(meta.get(\"height\",\"\")),\n",
    "            str(meta.get(\"width\",\"\")),\n",
    "            str(meta.get(\"decode_chunk_size\",\"\")),\n",
    "            str(meta.get(\"wall_time_sec\",\"\")),\n",
    "            str(meta.get(\"video_duration_sec\",\"\")),\n",
    "            str(meta.get(\"RTF\",\"\")),\n",
    "            '\"' + meta.get(\"out_mp4\",\"\") + '\"',\n",
    "        ]) + \"\\n\"\n",
    "        with METRICS_CSV.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line)\n",
    "        print(\"\\n✅ Video generated:\", meta[\"out_mp4\"])\n",
    "        print(\"   Metrics appended to:\", METRICS_CSV)\n",
    "    else:\n",
    "        print(\"\\n❌ Generation failed.\")\n",
    "        print(\"   Error:\", meta.get(\"error\",\"unknown\"))\n",
    "        print(\"   Tried combos (tail):\", meta.get(\"tried\",\"\")[-6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33239a6c-1c37-4a7f-b077-7926cea758d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU-exclusive SVD wrapper ready. Call svd_gpu_one_shot(<image>, prompt='...').\n",
      "   Notes: GPU math only; weights offloaded to RAM; VRAM cleared before/after; expandable_segments enabled.\n"
     ]
    }
   ],
   "source": [
    "# Week11-HO-2 · Cell — GPU-exclusive gate + expandable segments + VRAM clears (wraps mem-safe SVD)\n",
    "# Requires: gen_i2v_svd_memsafe(...) from the previous cell.\n",
    "# Use: meta = svd_gpu_one_shot(\"/path/to/image.png\", prompt=\"...\")\n",
    "\n",
    "import os, gc, fcntl, time\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- 1) CUDA allocator: expandable segments (set before heavy allocations) ---\n",
    "# Also a couple of conservative knobs for fragmentation & GC.\n",
    "alloc_conf = os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", \"\")\n",
    "wanted = \"expandable_segments:True,max_split_size_mb:128,garbage_collection_threshold:0.6\"\n",
    "if wanted not in alloc_conf:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (alloc_conf + (\",\" if alloc_conf else \"\") + wanted)\n",
    "\n",
    "# --- 2) Hard VRAM clear helper ---\n",
    "def _hard_clear_vram():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "# --- 3) Exclusive GPU gate (single-process/system-wide) ---\n",
    "@contextmanager\n",
    "def gpu_gate(lock_file=\"/tmp/gpu0_semaphore.lock\"):\n",
    "    Path(lock_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    fd = open(lock_file, \"w\")\n",
    "    try:\n",
    "        fcntl.flock(fd, fcntl.LOCK_EX)   # wait for exclusive access\n",
    "        yield\n",
    "    finally:\n",
    "        try: fcntl.flock(fd, fcntl.LOCK_UN)\n",
    "        finally: fd.close()\n",
    "\n",
    "# --- 4) Wrapper that enforces: GPU-only compute, minimal batch, offload to RAM, strict clears ---\n",
    "def svd_gpu_one_shot(\n",
    "    image_path: str,\n",
    "    prompt: str = \"\",\n",
    "    *,\n",
    "    # strong GPU-first stance:\n",
    "    device_pref: str = \"cuda\",\n",
    "    # minimal/\"small batch\" behavior is handled by mem-safe backoff; start sensibly:\n",
    "    height: int = 576, width: int = 1024,\n",
    "    frames: int = 24, fps: int = 8,\n",
    "    steps: int = 18,\n",
    "    motion: int = 127,\n",
    "    noise: float = 0.02,\n",
    "    seed: int = 42,\n",
    "    # explicit preference for bf16 + sequential offload (weights in RAM, compute on GPU):\n",
    "    dtype_pref: str = \"bf16\",\n",
    "    offload_pref: str = \"sequential\"\n",
    "):\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"ok\": False, \"error\": \"CUDA not available\"}\n",
    "\n",
    "    # single-op GPU critical section\n",
    "    with gpu_gate():\n",
    "        _hard_clear_vram()\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            # Call the mem-safe generator; it will keep everything on GPU for math,\n",
    "            # offload weights to RAM between layers, and auto-backoff on OOM.\n",
    "            meta = gen_i2v_svd_memsafe(\n",
    "                image_path,\n",
    "                prompt=prompt,\n",
    "                dtype_pref=dtype_pref,\n",
    "                offload_pref=offload_pref,\n",
    "                device_pref=device_pref,\n",
    "                height=height, width=width,\n",
    "                num_frames=frames, fps=fps,\n",
    "                num_inference_steps=steps,\n",
    "                motion_bucket_id=motion, noise_aug_strength=noise,\n",
    "            )\n",
    "        finally:\n",
    "            # hard VRAM clear to avoid overlap with the next big op/model\n",
    "            _hard_clear_vram()\n",
    "        t1 = time.time()\n",
    "\n",
    "    # Annotate with timing + allocator settings for traceability\n",
    "    meta = meta if isinstance(meta, dict) else {\"ok\": False, \"error\": \"unknown_return\"}\n",
    "    meta.setdefault(\"ok\", False)\n",
    "    meta[\"wall_total_sec\"] = round(t1 - t0, 3)\n",
    "    meta[\"allocator_conf\"] = os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\", \"\")\n",
    "    meta[\"gpu_gate\"] = True\n",
    "    return meta\n",
    "\n",
    "print(\"✅ GPU-exclusive SVD wrapper ready. Call svd_gpu_one_shot(<image>, prompt='...').\")\n",
    "print(\"   Notes: GPU math only; weights offloaded to RAM; VRAM cleared before/after; expandable_segments enabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc34ec5-eec7-4da2-b535-dd78e00fb3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caffdad49e3c483c87ac841190bff947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_213033.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 16, 'chunk': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca92796da3b541e08eacdfbc16439e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_213350.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7527302fb0fa4aca9b0b4b801ea98c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_213715.mp4\n",
      "    used: {'dtype': 'torch.float16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da42ed4d9ca74a26883a6b88e3e9fe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 15.01 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 84.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m meta = \u001b[43msvd_gpu_one_shot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/Safe & Sound _ A Renter-Friendly Guide to Home Repair/img-112-085.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mShort instructional clip focusing on the faucet spout and drip area; subtle motion.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m meta\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36msvd_gpu_one_shot\u001b[39m\u001b[34m(image_path, prompt, device_pref, height, width, frames, fps, steps, motion, noise, seed, dtype_pref, offload_pref)\u001b[39m\n\u001b[32m     62\u001b[39m t0 = time.time()\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call the mem-safe generator; it will keep everything on GPU for math,\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# offload weights to RAM between layers, and auto-backoff on OOM.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     meta = \u001b[43mgen_i2v_svd_memsafe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmotion_bucket_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmotion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_aug_strength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# hard VRAM clear to avoid overlap with the next big op/model\u001b[39;00m\n\u001b[32m     79\u001b[39m     _hard_clear_vram()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mgen_i2v_svd_memsafe\u001b[39m\u001b[34m(image_path, prompt, seed, height, width, num_frames, fps, num_inference_steps, motion_bucket_id, noise_aug_strength, decode_chunk_size, dtype_pref, offload_pref, device_pref, model_id)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m off \u001b[38;5;129;01min\u001b[39;00m offload_candidates:\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# Load/reload the pipeline per (dtype, offload) combo for clean memory\u001b[39;00m\n\u001b[32m    159\u001b[39m     _torch_gc()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     pipe, device, dtype_str = \u001b[43mload_svd_memsafe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_pref\u001b[49m\u001b[43m=\u001b[49m\u001b[43moff\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# enable chunked decoding by default; we'll vary the chunk size in the loop\u001b[39;00m\n\u001b[32m    167\u001b[39m     ok_this_combo = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mload_svd_memsafe\u001b[39m\u001b[34m(model_id, device_pref, dtype_pref, offload_pref)\u001b[39m\n\u001b[32m     83\u001b[39m pipe = StableVideoDiffusionPipeline.from_pretrained(\n\u001b[32m     84\u001b[39m     model_id,\n\u001b[32m     85\u001b[39m     torch_dtype=dtype,\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# NOTE: don't pass variant=\"fp16\" so we freely cast to bf16 if needed.\u001b[39;00m\n\u001b[32m     87\u001b[39m )\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# move/prepare\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# memory savers\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m: pipe.enable_attention_slicing(\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:418\u001b[39m, in \u001b[36mDiffusionPipeline.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    414\u001b[39m     logger.warning(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been loaded in 8bit and moving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m     )\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    421\u001b[39m     module.dtype == torch.float16\n\u001b[32m    422\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    423\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[32m    424\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[32m    425\u001b[39m ):\n\u001b[32m    426\u001b[39m     logger.warning(\n\u001b[32m    427\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    431\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/core-rag/lib/python3.11/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 37.38 MiB is free. Including non-PyTorch memory, this process has 15.01 GiB memory in use. Of the allocated memory 14.63 GiB is allocated by PyTorch, and 84.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "meta = svd_gpu_one_shot(\n",
    "    \"/home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/Safe & Sound _ A Renter-Friendly Guide to Home Repair/img-112-085.png\",\n",
    "    prompt=\"Short instructional clip focusing on the faucet spout and drip area; subtle motion.\"\n",
    ")\n",
    "meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94cc404a-107a-4242-b5a6-552b1f2c9d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'faucet spout' | text pages: 16 | candidate images kept: 3\n",
      "  [1] q=1.018 /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-012-033.png  (doc=1001 do-it-yourself hints & tips  tricks.pdf p12)  entropy=6.82 dom=0.024 grad=3022.0\n",
      "  [2] q=0.990 /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-013-037.png  (doc=1001 do-it-yourself hints & tips  tricks.pdf p13)  entropy=6.33 dom=0.038 grad=3091.6\n",
      "  [3] q=0.892 /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-012-035.png  (doc=1001 do-it-yourself hints & tips  tricks.pdf p12)  entropy=5.93 dom=0.308 grad=66992.7\n",
      "\n",
      "Chosen image: /home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-012-033.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed507cc5edc46ee86fb07b490a6491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_215138.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9243bd3f73a742a99c3f6e586ca9c842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_215831.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e3a3615e36439a980d48eff443224e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_220523.mp4\n",
      "    used: {'dtype': 'torch.float16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec5b615fd9146afaeaaaae5f3b1d4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n",
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_221619.mp4\n",
      "    used: {'dtype': 'torch.float32', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 2}\n",
      "\n",
      "✅ Video generated: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251103_221619.mp4\n",
      "   Metrics appended to: /home/manny-buff/projects/capstone/week11-hw/artifacts/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Week11-HO-2 · Cell — Query-time image quality filter + dedup + one-shot SVD video\n",
    "# Assumes:\n",
    "#   - svd_gpu_one_shot(...) is defined (GPU-exclusive wrapper)\n",
    "#   - Graph v2 JSON + TF-IDF index exist from earlier steps\n",
    "# No new installs required.\n",
    "\n",
    "import os, re, json, math, hashlib\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ---------- Paths ----------\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "TFIDF_INDEX   = W11 / \"artifacts\" / \"graph\" / \"index\" / \"tfidf_index.json\"\n",
    "RAW_TEXT_ROOT = W11 / \"artifacts\" / \"graph\" / \"raw_text\"\n",
    "V2_GRAPH_JSON = W11 / \"artifacts\" / \"graph_v2\" / \"graph\" / \"graph.json\"\n",
    "METRICS_CSV   = W11 / \"artifacts\" / \"metrics.csv\"\n",
    "VIS_PROMPT    = (\"Short instructional clip showing the faucet spout area; subtle motion; \"\n",
    "                 \"neutral lighting; emphasize where drips occur and what to inspect.\")\n",
    "\n",
    "# ---------- TF-IDF search (standalone) ----------\n",
    "WORD = re.compile(r\"[a-zA-Z0-9]+(?:'[a-z0-9]+)?\")\n",
    "STOP = set(\"a an and are as at be by for from has have in is it its of on or that the to with your you we he she they them their our\".split())\n",
    "\n",
    "def tokenize(text): return [w.lower() for w in WORD.findall(text) if w.lower() not in STOP and len(w) > 1]\n",
    "\n",
    "def load_json(p: Path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {p}\")\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "tf = load_json(TFIDF_INDEX)\n",
    "\n",
    "def search_pages_local(query: str, top_k: int = 10):\n",
    "    q = tokenize(query)\n",
    "    if not q: return []\n",
    "    q_tf = Counter(q)\n",
    "    q_vec = {}\n",
    "    for t, c in q_tf.items():\n",
    "        idf = tf[\"idf\"].get(t, 0.0)\n",
    "        w = (c / max(1, len(q))) * idf\n",
    "        if w > 0: q_vec[t] = w\n",
    "    q_norm = math.sqrt(sum(v*v for v in q_vec.values())) or 1.0\n",
    "\n",
    "    scores = []\n",
    "    for i in range(tf[\"N\"]):\n",
    "        tf_top = tf[\"tf_top\"][i]\n",
    "        dot = 0.0\n",
    "        for t, qw in q_vec.items():\n",
    "            if t in tf_top:\n",
    "                dw = (tf_top[t]/200.0) * tf[\"idf\"].get(t, 0.0)\n",
    "                dot += qw * dw\n",
    "        denom = (q_norm * (tf[\"norms\"][i] or 1.0))\n",
    "        s = dot/denom if denom else 0.0\n",
    "        if s > 0: scores.append((s, i))\n",
    "    scores.sort(reverse=True)\n",
    "    hits = scores[:top_k]\n",
    "    out = []\n",
    "    for s, i in hits:\n",
    "        meta = tf[\"docs\"][i]  # {doc, stem, page_idx}\n",
    "        raw = RAW_TEXT_ROOT / meta[\"stem\"] / f\"page_{meta['page_idx']:04d}.txt\"\n",
    "        snippet = raw.read_text(encoding=\"utf-8\", errors=\"ignore\")[:360] if raw.exists() else \"\"\n",
    "        out.append({\"score\": round(float(s), 6), \"doc\": meta[\"doc\"], \"stem\": meta[\"stem\"], \"page\": meta[\"page_idx\"], \"snippet\": snippet.replace(\"\\n\",\" \")})\n",
    "    return out\n",
    "\n",
    "# ---------- Graph v2 page→image ----------\n",
    "g2 = load_json(V2_GRAPH_JSON)\n",
    "nodes = {n[\"id\"]: n for n in g2[\"nodes\"]}\n",
    "adj   = defaultdict(list)\n",
    "for e in g2[\"edges\"]:\n",
    "    adj[e[\"u\"]].append((e[\"v\"], e))\n",
    "    adj[e[\"v\"]].append((e[\"u\"], e))\n",
    "\n",
    "def page_node_id(doc: str, page_idx: int): return f\"doc::{doc}::p{page_idx}\"\n",
    "\n",
    "def page_images(doc: str, page_idx: int):\n",
    "    pid = page_node_id(doc, page_idx)\n",
    "    if pid not in nodes: return []\n",
    "    out = []\n",
    "    for nbr, _ in adj[pid]:\n",
    "        nd = nodes.get(nbr, {})\n",
    "        if nd.get(\"kind\") == \"image\":\n",
    "            p = Path(nd.get(\"path\",\"\"))\n",
    "            if p.exists():\n",
    "                out.append({\"path\": str(p), \"w\": nd.get(\"width\",0), \"h\": nd.get(\"height\",0)})\n",
    "    return out\n",
    "\n",
    "# ---------- Classical CV quality metrics + aHash dedup ----------\n",
    "def pil_to_gray_arr(p: Path, target_max=256):\n",
    "    im = Image.open(p).convert(\"L\")\n",
    "    # downscale for stable metrics\n",
    "    w, h = im.size\n",
    "    scale = min(1.0, target_max / float(max(w, h)))\n",
    "    if scale < 1.0:\n",
    "        im = im.resize((max(1,int(w*scale)), max(1,int(h*scale))), Image.BICUBIC)\n",
    "    return np.asarray(im, dtype=np.uint8)\n",
    "\n",
    "def entropy(arr: np.ndarray):\n",
    "    hist = np.bincount(arr.flatten(), minlength=256).astype(\"float32\")\n",
    "    p = hist / (hist.sum() + 1e-8)\n",
    "    return float(-(p[p>0] * np.log2(p[p>0])).sum())\n",
    "\n",
    "def dominant_fraction(arr: np.ndarray):\n",
    "    hist = np.bincount(arr.flatten(), minlength=256).astype(\"float32\")\n",
    "    return float(hist.max() / (hist.sum() + 1e-8))\n",
    "\n",
    "def _conv2(img: np.ndarray, k: np.ndarray):\n",
    "    # simple 3x3 conv, edge-pad; fine at 256px\n",
    "    pad = 1\n",
    "    p = np.pad(img.astype(\"float32\"), pad, mode=\"edge\")\n",
    "    out = (k[0,0]*p[0:-2,0:-2] + k[0,1]*p[0:-2,1:-1] + k[0,2]*p[0:-2,2:] +\n",
    "           k[1,0]*p[1:-1,0:-2] + k[1,1]*p[1:-1,1:-1] + k[1,2]*p[1:-1,2:] +\n",
    "           k[2,0]*p[2:,  0:-2] + k[2,1]*p[2:,  1:-1] + k[2,2]*p[2:,  2:])\n",
    "    return out\n",
    "\n",
    "def gradient_var(arr: np.ndarray):\n",
    "    sobel_x = np.array([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=np.float32)\n",
    "    sobel_y = sobel_x.T\n",
    "    gx = _conv2(arr, sobel_x)\n",
    "    gy = _conv2(arr, sobel_y)\n",
    "    g = np.hypot(gx, gy)\n",
    "    return float(g.var())\n",
    "\n",
    "def ahash_hex(arr: np.ndarray, size=8):\n",
    "    im = Image.fromarray(arr).resize((size, size), Image.BICUBIC)\n",
    "    a = np.asarray(im, dtype=np.float32)\n",
    "    mean = a.mean()\n",
    "    bits = (a >= mean).astype(np.uint8).flatten()\n",
    "    # pack to hex string (64 bits for 8x8)\n",
    "    v = 0\n",
    "    for b in bits:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:016x}\"\n",
    "\n",
    "def quality_score(p: Path):\n",
    "    arr = pil_to_gray_arr(p, target_max=256)\n",
    "    h, w = arr.shape\n",
    "    ent = entropy(arr)                 # ~0..8\n",
    "    dom = dominant_fraction(arr)       # near 1.0 means near-blank/flat\n",
    "    gvar = gradient_var(arr)           # higher => sharper / more detail\n",
    "    area = float(h*w)\n",
    "\n",
    "    # Normalize into roughly 0..1 bands (hand-tuned, conservative)\n",
    "    ent_n  = min(1.0, ent / 7.0)\n",
    "    gvar_n = min(1.0, gvar / 1200.0)\n",
    "    area_n = min(1.0, area / float(576*1024))  # prefer ≥576x1024-ish\n",
    "    flat_n = max(0.0, 1.0 - (dom - 0.6)/0.35)  # penalize if dom > ~0.95\n",
    "\n",
    "    score = (0.40*gvar_n + 0.35*ent_n + 0.15*area_n + 0.10*flat_n)\n",
    "    return {\n",
    "        \"score\": float(score),\n",
    "        \"entropy\": float(ent),\n",
    "        \"dominant_frac\": float(dom),\n",
    "        \"grad_var\": float(gvar),\n",
    "        \"area\": int(area),\n",
    "        \"ahash\": ahash_hex(arr)\n",
    "    }\n",
    "\n",
    "def good_enough(metrics, min_entropy=3.0, max_dom=0.985, min_grad_var=150.0, min_area=128*128):\n",
    "    if metrics[\"area\"] < min_area: return False\n",
    "    if metrics[\"entropy\"] < min_entropy: return False\n",
    "    if metrics[\"dominant_frac\"] > max_dom: return False   # near-blank\n",
    "    if metrics[\"grad_var\"] < min_grad_var: return False   # very blurry\n",
    "    return True\n",
    "\n",
    "# ---------- Selection for a query ----------\n",
    "def select_images_for_query(query: str, top_pages=12, per_page=4, topM=3):\n",
    "    hits = search_pages_local(query, top_k=top_pages)\n",
    "    # Collect candidates across pages\n",
    "    cand = []\n",
    "    seen_hashes = set()\n",
    "    for h in hits:\n",
    "        imgs = page_images(h[\"doc\"], h[\"page\"])[:per_page]\n",
    "        for it in imgs:\n",
    "            p = Path(it[\"path\"])\n",
    "            try:\n",
    "                m = quality_score(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not good_enough(m):           # quality gate\n",
    "                continue\n",
    "            if m[\"ahash\"] in seen_hashes:    # dedup\n",
    "                continue\n",
    "            seen_hashes.add(m[\"ahash\"])\n",
    "            cand.append({\"path\": str(p), \"w\": it[\"w\"], \"h\": it[\"h\"], \"q\": m[\"score\"], \"doc\": h[\"doc\"], \"page\": h[\"page\"], \"metrics\": m})\n",
    "    cand.sort(key=lambda x: (x[\"q\"], x[\"w\"]*x[\"h\"]), reverse=True)\n",
    "    return cand[:topM], hits\n",
    "\n",
    "# ---------- Ensure metrics CSV exists ----------\n",
    "if not METRICS_CSV.exists():\n",
    "    METRICS_CSV.write_text(\n",
    "        \"timestamp,pipeline,domain,query,source_doc,source_page,image_path,model,frames,fps,steps,motion,noise,height,width,chunk,wall_s,video_s,RTF,out_mp4\\n\"\n",
    "    )\n",
    "\n",
    "# ---------- Run once for 'faucet spout' ----------\n",
    "QUERY = \"faucet spout\"\n",
    "candidates, hits = select_images_for_query(QUERY, top_pages=16, per_page=6, topM=3)\n",
    "print(f\"Query: {QUERY!r} | text pages: {len(hits)} | candidate images kept: {len(candidates)}\")\n",
    "for i, c in enumerate(candidates, 1):\n",
    "    print(f\"  [{i}] q={c['q']:.3f} {c['path']}  (doc={c['doc']} p{c['page']})  \"\n",
    "          f\"entropy={c['metrics']['entropy']:.2f} dom={c['metrics']['dominant_frac']:.3f} grad={c['metrics']['grad_var']:.1f}\")\n",
    "\n",
    "chosen = candidates[0] if candidates else None\n",
    "if not chosen:\n",
    "    print(\"No suitable images passed quality checks. Consider relaxing thresholds or widening search.\")\n",
    "else:\n",
    "    print(\"\\nChosen image:\", chosen[\"path\"])\n",
    "    try:\n",
    "        meta = svd_gpu_one_shot(chosen[\"path\"], prompt=VIS_PROMPT)  # GPU-exclusive + clears between ops\n",
    "    except NameError as e:\n",
    "        raise RuntimeError(\"svd_gpu_one_shot(...) not found. Run the GPU wrapper cell first.\") from e\n",
    "\n",
    "    if meta.get(\"ok\"):\n",
    "        # append metrics row\n",
    "        line = \",\".join([\n",
    "            datetime.now().isoformat(),\n",
    "            \"Image2Video-SVD-memsafe\",\n",
    "            \"home_repair\",\n",
    "            '\"' + QUERY.replace('\"', \"'\") + '\"',\n",
    "            '\"' + chosen[\"doc\"] + '\"',\n",
    "            str(chosen[\"page\"]),\n",
    "            '\"' + chosen[\"path\"] + '\"',\n",
    "            meta.get(\"model\",\"\"),\n",
    "            str(meta.get(\"frames\",\"\")),\n",
    "            str(meta.get(\"fps\",\"\")),\n",
    "            str(meta.get(\"steps\",\"\")),\n",
    "            str(meta.get(\"motion_bucket_id\",\"\")),\n",
    "            str(meta.get(\"noise_aug_strength\",\"\")),\n",
    "            str(meta.get(\"height\",\"\")),\n",
    "            str(meta.get(\"width\",\"\")),\n",
    "            str(meta.get(\"decode_chunk_size\",\"\")),\n",
    "            str(meta.get(\"wall_time_sec\",\"\")),\n",
    "            str(meta.get(\"video_duration_sec\",\"\")),\n",
    "            str(meta.get(\"RTF\",\"\")),\n",
    "            '\"' + meta.get(\"out_mp4\",\"\") + '\"',\n",
    "        ]) + \"\\n\"\n",
    "        with METRICS_CSV.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(line)\n",
    "        print(\"\\n✅ Video generated:\", meta[\"out_mp4\"])\n",
    "        print(\"   Metrics appended to:\", METRICS_CSV)\n",
    "    else:\n",
    "        print(\"\\n❌ Generation failed.\")\n",
    "        print(\"   Error:\", meta.get(\"error\",\"unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7224a8c0-60ac-40a6-88fc-1dd47932e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week11-HO-2 · Cell — Qwen2.5-VL (GPU-gated) with K=2 images + hardcoded fallback\n",
    "# Assumes:\n",
    "#   - gpu_gate(...) and _hard_clear_vram() exist (from your SVD GPU wrapper cell).\n",
    "#   - select_images_for_query(query, top_pages, per_page, topM) exists (quality+dedup filter cell).\n",
    "#   - Transformers>=4.41,<4.45 installed (env already pinned); bf16 preferred on Ada.\n",
    "# Outputs:\n",
    "#   - artifacts/vlm/answer_<ts>.json (and .md)\n",
    "#   - Append one row to artifacts/metrics.csv\n",
    "\n",
    "import os, gc, time, json, platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoProcessor, AutoConfig\n",
    "\n",
    "# ---- Config & paths ----\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "ART_VLM = W11 / \"artifacts\" / \"vlm\"\n",
    "ART_VLM.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_CSV = W11 / \"artifacts\" / \"metrics.csv\"\n",
    "\n",
    "QUERY = \"faucet spout\"\n",
    "K = 2  # try 2 images, fallback to 1 if needed\n",
    "\n",
    "# Fallback image (your good faucet diagram)\n",
    "FALLBACK_IMG = Path(\"/home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/Safe & Sound _ A Renter-Friendly Guide to Home Repair/img-112-085.png\")\n",
    "\n",
    "# Prefer Qwen2.5-VL-2B-Instruct; fallback to Qwen2-VL-2B-Instruct if that's the folder name on disk\n",
    "MODEL_DIRS = [\n",
    "    Path(\"~/projects/capstone/hw-rag/models/Qwen2.5-VL-2B-Instruct\").expanduser(),\n",
    "    Path(\"~/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct\").expanduser(),\n",
    "]\n",
    "MODEL_ID = None\n",
    "for p in MODEL_DIRS:\n",
    "    if p.exists():\n",
    "        MODEL_ID = str(p)\n",
    "        break\n",
    "if MODEL_ID is None:\n",
    "    raise FileNotFoundError(\"Qwen2.x-VL-2B-Instruct model folder not found under ~/projects/capstone/hw-rag/models/\")\n",
    "\n",
    "# Compose a short, grading-friendly prompt\n",
    "SYSTEM_HINT = (\n",
    "    \"You are a concise home-repair assistant. Use the provided images and snippet to explain how to diagnose and \"\n",
    "    \"reduce a faucet spout drip (leak at the spout). Limit to numbered steps (≤8), tools, and cautions. \"\n",
    "    \"Cite page index like [pXX] when a step comes from the snippet.\"\n",
    ")\n",
    "\n",
    "USER_INSTRUCT = (\n",
    "    \"From the context and images, give clear steps to diagnose and fix a drip at the faucet spout. \"\n",
    "    \"Assume a common compression or cartridge faucet.\"\n",
    ")\n",
    "\n",
    "# ---- Helpers: assemble K images (quality filter first, then fallback) ----\n",
    "def collect_images_for_query(query: str, k: int):\n",
    "    paths = []\n",
    "    hit_meta = []\n",
    "    try:\n",
    "        cands, hits = select_images_for_query(query, top_pages=16, per_page=6, topM=6)  # from your prior cell\n",
    "        for c in cands:\n",
    "            if len(paths) >= k: break\n",
    "            p = Path(c[\"path\"])\n",
    "            if p.exists():\n",
    "                paths.append(p)\n",
    "                hit_meta.append({\"doc\": c[\"doc\"], \"page\": c[\"page\"], \"path\": str(p)})\n",
    "    except NameError:\n",
    "        # The filter cell wasn't run; fall back immediately\n",
    "        pass\n",
    "\n",
    "    # If too few, append fallback(s)\n",
    "    if len(paths) < k and FALLBACK_IMG.exists():\n",
    "        while len(paths) < k:\n",
    "            paths.append(FALLBACK_IMG)\n",
    "            hit_meta.append({\"doc\": \"fallback\", \"page\": -1, \"path\": str(FALLBACK_IMG)})\n",
    "    return paths[:k], hit_meta[:k]\n",
    "\n",
    "# --- helper: choose a local VLM model; prefer Qwen2-VL, else InternVL ---\n",
    "# --- DROP-IN: robust discovery of local VLMs (Qwen/Intern, any naming) ---\n",
    "def discover_local_vlms(models_root=\"~/projects/capstone/hw-rag/models\"):\n",
    "    \"\"\"\n",
    "    Scan one level under models_root for model dirs with config.json.\n",
    "    Returns a list of dicts with: path, name, model_type, is_vl, score (higher is preferred).\n",
    "    \"\"\"\n",
    "    import json, re\n",
    "    from pathlib import Path\n",
    "\n",
    "    root = Path(models_root).expanduser()\n",
    "    found = []\n",
    "    if not root.exists():\n",
    "        return found\n",
    "\n",
    "    # one level deep\n",
    "    for d in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "        cfg = d / \"config.json\"\n",
    "        if not cfg.exists():\n",
    "            continue\n",
    "        model_type = \"\"\n",
    "        cfg_json = {}\n",
    "        try:\n",
    "            cfg_json = json.loads(cfg.read_text())\n",
    "            model_type = str(cfg_json.get(\"model_type\", \"\")).lower()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        name = d.name.lower()\n",
    "        # heuristic: look for signs of VL\n",
    "        jl = json.dumps(cfg_json).lower()\n",
    "        looks_vl = any(k in jl for k in [\n",
    "            \"vision_config\", \"vision_tower\", \"mm_projector\", \"multi_modal_projector\",\n",
    "            \"image_token_index\", \"image_size\", \"image_grid_pinpoints\"\n",
    "        ])\n",
    "\n",
    "        # classify model family\n",
    "        is_qwen = (\"qwen\" in name) or (\"qwen\" in model_type)\n",
    "        is_intern = any(x in name for x in [\"intern\", \"internvl\", \"intern3\", \"intern3_5\"]) or \\\n",
    "                    any(x in model_type for x in [\"intern\", \"internvl\", \"intern3\", \"intern3_5\"])\n",
    "\n",
    "        # score preference: Qwen2.5-VL > Qwen2-VL > Intern3.5-VL > other VL\n",
    "        score = 0\n",
    "        # precise matches first\n",
    "        if re.search(r\"qwen2\\.5.*vl\", name) or re.search(r\"qwen2\\.5.*vl\", model_type):\n",
    "            score = 100\n",
    "        elif re.search(r\"qwen2.*vl\", name) or re.search(r\"qwen2.*vl\", model_type):\n",
    "            score = 90\n",
    "        elif re.search(r\"intern(3|3_5).*vl\", name) or re.search(r\"intern(3|3_5).*vl\", model_type):\n",
    "            score = 80\n",
    "        elif is_qwen and looks_vl:\n",
    "            score = 70\n",
    "        elif is_intern and looks_vl:\n",
    "            score = 60\n",
    "        elif looks_vl:\n",
    "            score = 50\n",
    "\n",
    "        found.append({\n",
    "            \"path\": str(d),\n",
    "            \"name\": d.name,\n",
    "            \"model_type\": model_type or \"unknown\",\n",
    "            \"is_vl\": bool(looks_vl),\n",
    "            \"score\": score,\n",
    "        })\n",
    "    # highest score first\n",
    "    found.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    # print short report\n",
    "    print(\"Discovered VLM candidates:\")\n",
    "    for x in found:\n",
    "        print(f\" - {x['name']:40s}  type={x['model_type']:12s}  VL={x['is_vl']}  score={x['score']:>3d}\")\n",
    "    return found\n",
    "\n",
    "\n",
    "# --- DROP-IN: pick the best local VLM using discovery (no version assumptions) ---\n",
    "def _pick_local_vlm_model():\n",
    "    cands = discover_local_vlms()\n",
    "    # take first that looks VL and has a positive score; else first any\n",
    "    for c in cands:\n",
    "        if c[\"is_vl\"] and c[\"score\"] > 0:\n",
    "            return (c[\"path\"], c[\"model_type\"])\n",
    "    if cands:\n",
    "        return (cands[0][\"path\"], cands[0][\"model_type\"])\n",
    "    raise FileNotFoundError(\"No model folders with config.json found under ~/projects/capstone/hw-rag/models\")\n",
    "\n",
    "\n",
    "# --- DROP-IN: VLM answer with discovery + per-candidate error logging (GPU-gated, backoffs intact) ---\n",
    "def qwen_vl_answer(query: str, k_images=2, max_new=256, model_override: str | None = None):\n",
    "    \"\"\"\n",
    "    GPU-gated VLM answer with K images (fallback to 1).\n",
    "    Robust multi-strategy loader:\n",
    "      - Qwen2-VL: try Qwen2VLProcessor+Qwen2VLForConditionalGeneration, then Auto*.\n",
    "      - InternVL:  try AutoProcessor+AutoModelForCausalLM, then AutoProcessor+AutoModel (if .generate exists).\n",
    "    Saves JSON/MD + appends metrics; logs per-candidate load errors.\n",
    "    \"\"\"\n",
    "    import platform, time, json, torch, traceback\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    from PIL import Image\n",
    "    from transformers import AutoProcessor, AutoModelForCausalLM, AutoModel\n",
    "\n",
    "    # ---- pick images (quality filter first, then your hardcoded fallback) ----\n",
    "    img_paths, _ = collect_images_for_query(query, k_images)\n",
    "    if not img_paths:\n",
    "        if FALLBACK_IMG.exists():\n",
    "            img_paths = [FALLBACK_IMG]\n",
    "        else:\n",
    "            raise RuntimeError(\"No images available (even fallback path missing).\")\n",
    "    images = [Image.open(p).convert(\"RGB\") for p in img_paths]\n",
    "\n",
    "    # dtype preference\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    dtype = torch.bfloat16 if use_bf16 else (torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "    # ---- candidate list via discovery (or override) ----\n",
    "    tried, load_errors = [], []\n",
    "    if model_override:\n",
    "        candidates = [{\"path\": str(Path(model_override).expanduser()), \"name\": Path(model_override).expanduser().name, \"model_type\": \"\"}]\n",
    "    else:\n",
    "        candidates = discover_local_vlms()\n",
    "\n",
    "    def _load_multi_strategy(mid: str):\n",
    "        \"\"\"\n",
    "        Try several strategies to load model/processor for 'mid'.\n",
    "        Returns (processor, model, strategy_name) or raises the last Exception.\n",
    "        \"\"\"\n",
    "        last_exc = None\n",
    "\n",
    "        # Try Qwen native class first (on newer transformers)\n",
    "        try:\n",
    "            from transformers import Qwen2VLProcessor, Qwen2VLForConditionalGeneration  # type: ignore\n",
    "            proc = Qwen2VLProcessor.from_pretrained(mid, trust_remote_code=True, local_files_only=True)\n",
    "            mdl  = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                mid, torch_dtype=dtype, low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True, local_files_only=True\n",
    "            ).to(\"cuda\")\n",
    "            return proc, mdl, \"Qwen2VL-native\"\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "\n",
    "        # AutoProcessor + AutoModelForCausalLM (common remote-code path)\n",
    "        try:\n",
    "            proc = AutoProcessor.from_pretrained(mid, trust_remote_code=True, local_files_only=True)\n",
    "            mdl  = AutoModelForCausalLM.from_pretrained(\n",
    "                mid, torch_dtype=dtype, low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True, local_files_only=True\n",
    "            ).to(\"cuda\")\n",
    "            return proc, mdl, \"AutoModelForCausalLM\"\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "\n",
    "        # AutoProcessor + AutoModel (some VL repos expose generate on a base class)\n",
    "        try:\n",
    "            proc = AutoProcessor.from_pretrained(mid, trust_remote_code=True, local_files_only=True)\n",
    "            mdl  = AutoModel.from_pretrained(\n",
    "                mid, torch_dtype=dtype, low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True, local_files_only=True\n",
    "            ).to(\"cuda\")\n",
    "            # ensure we can generate\n",
    "            if not hasattr(mdl, \"generate\"):\n",
    "                raise RuntimeError(\"Loaded AutoModel but it has no .generate()\")\n",
    "            return proc, mdl, \"AutoModel(with generate)\"\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            raise last_exc\n",
    "\n",
    "    # ---- GPU-gated run ----\n",
    "    with gpu_gate():\n",
    "        _hard_clear_vram()\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            processor = model = None\n",
    "            used_strategy = None\n",
    "\n",
    "            # iterate candidates\n",
    "            for c in candidates:\n",
    "                mid = c[\"path\"] if isinstance(c, dict) else str(c)\n",
    "                try:\n",
    "                    processor, model, used_strategy = _load_multi_strategy(mid)\n",
    "                    tried.append(f\"{mid} [{used_strategy}]\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    tried.append(mid)\n",
    "                    load_errors.append({\"model\": mid, \"error\": f\"{type(e).__name__}: {e}\"})\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "            if model is None:\n",
    "                # write the detailed failure log\n",
    "                logp = (W11 / \"artifacts\" / \"vlm\" / \"load_failures.log\")\n",
    "                logp.parent.mkdir(parents=True, exist_ok=True)\n",
    "                logp.write_text(json.dumps({\"tried\": tried, \"errors\": load_errors}, indent=2), encoding=\"utf-8\")\n",
    "                raise RuntimeError(\"No VLM could be loaded. See artifacts/vlm/load_failures.log for details.\")\n",
    "\n",
    "            # perf toggles\n",
    "            try: torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            except Exception: pass\n",
    "            try: torch.set_float32_matmul_precision(\"high\")\n",
    "            except Exception: pass\n",
    "\n",
    "            SYSTEM_HINT = (\n",
    "                \"You are a concise home-repair assistant. Use provided images to explain how to diagnose \"\n",
    "                \"and reduce a faucet spout drip. Number steps (≤8); include tools & cautions.\"\n",
    "            )\n",
    "            USER_INSTRUCT = (\n",
    "                \"From the context and images, give clear steps to diagnose and fix a drip at the faucet spout. \"\n",
    "                \"Assume a common compression or cartridge faucet.\"\n",
    "            )\n",
    "\n",
    "            def build_prompt(n_imgs):\n",
    "                if hasattr(processor, \"apply_chat_template\"):\n",
    "                    msg = [\n",
    "                        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_HINT}]},\n",
    "                        {\"role\": \"user\",   \"content\": [{\"type\": \"text\", \"text\": USER_INSTRUCT}] + [{\"type\":\"image\"} for _ in range(n_imgs)]},\n",
    "                    ]\n",
    "                    return processor.apply_chat_template(msg, add_generation_prompt=True, tokenize=False)\n",
    "                return SYSTEM_HINT + \"\\n\\n\" + USER_INSTRUCT\n",
    "\n",
    "            def prep(_imgs):\n",
    "                prompt_text = build_prompt(len(_imgs))\n",
    "                return processor(text=[prompt_text], images=_imgs, return_tensors=\"pt\").to(\"cuda\", dtype=dtype)\n",
    "\n",
    "            # backoffs: tokens ↓ then images ↓\n",
    "            attempts = [\n",
    "                {\"max_new_tokens\": max_new,     \"images\": images},\n",
    "                {\"max_new_tokens\": max_new//2,  \"images\": images},\n",
    "                {\"max_new_tokens\": max_new//2,  \"images\": images[:1]},\n",
    "                {\"max_new_tokens\": 128,         \"images\": images[:1]},\n",
    "            ]\n",
    "\n",
    "            out_text, used_attempt = None, None\n",
    "            for att in attempts:\n",
    "                try:\n",
    "                    inputs = prep(att[\"images\"])\n",
    "                    with torch.inference_mode(), torch.autocast(\"cuda\", dtype=dtype):\n",
    "                        gen_ids = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=att[\"max_new_tokens\"],\n",
    "                            do_sample=False,\n",
    "                            temperature=0.2,\n",
    "                            top_p=0.9,\n",
    "                        )\n",
    "                    out_text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0]\n",
    "                    used_attempt = att\n",
    "                    break\n",
    "                except torch.cuda.OutOfMemoryError:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "            t1 = time.time()\n",
    "        finally:\n",
    "            try: del model\n",
    "            except Exception: pass\n",
    "            _hard_clear_vram()\n",
    "\n",
    "    if out_text is None:\n",
    "        return {\"ok\": False, \"error\": \"VLM OOM after backoff\", \"images\": [str(p) for p in img_paths], \"tried_models\": tried}\n",
    "\n",
    "    # ---- save artifacts + metrics ----\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ART_VLM = (W11 / \"artifacts\" / \"vlm\"); ART_VLM.mkdir(parents=True, exist_ok=True)\n",
    "    out_json = ART_VLM / f\"answer_{ts}.json\"\n",
    "    out_md   = ART_VLM / f\"answer_{ts}.md\"\n",
    "    payload = {\n",
    "        \"ok\": True,\n",
    "        \"used_model\": tried[-1],\n",
    "        \"dtype\": str(dtype),\n",
    "        \"cuda\": torch.version.cuda if torch.cuda.is_available() else \"cpu\",\n",
    "        \"python\": platform.python_version(),\n",
    "        \"query\": query,\n",
    "        \"images\": [str(p) for p in img_paths][: len(used_attempt.get(\"images\", images))],\n",
    "        \"max_new_tokens\": used_attempt.get(\"max_new_tokens\", max_new),\n",
    "        \"answer\": out_text.strip(),\n",
    "        \"wall_time_sec\": round(t1 - t0, 3),\n",
    "        \"tried_models\": tried,\n",
    "        \"load_failures_logged\": bool(load_errors),\n",
    "    }\n",
    "    out_json.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    out_md.write_text(\n",
    "        f\"### VLM Answer ({ts})\\n\\n**Query:** {query}\\n\\n**Images used:**\\n\" +\n",
    "        \"\\n\".join([f\"- {p}\" for p in payload[\"images\"]]) + \"\\n\\n**Answer:**\\n\\n\" +\n",
    "        payload[\"answer\"] + \"\\n\", encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    if not METRICS_CSV.exists():\n",
    "        METRICS_CSV.write_text(\n",
    "            \"timestamp,pipeline,domain,query,source_doc,source_page,image_path,model,frames,fps,steps,motion,noise,height,width,chunk,wall_s,video_s,RTF,out_mp4\\n\"\n",
    "        )\n",
    "    with METRICS_CSV.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join([\n",
    "            datetime.now().isoformat(),\n",
    "            \"VLM (auto-discover, multi-strategy)\",\n",
    "            \"home_repair\",\n",
    "            '\"' + query.replace('\"', \"'\") + '\"',\n",
    "            \"n/a\",\"-1\",\n",
    "            '\"' + \";\".join(payload[\"images\"]) + '\"',\n",
    "            '\"' + payload[\"used_model\"] + '\"',\n",
    "            \"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\n",
    "            str(payload[\"wall_time_sec\"]),\n",
    "            \"\",\"\",\n",
    "            '\"\"'\n",
    "        ]) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ VLM answer saved: {out_json}  (used: {payload['used_model']})\")\n",
    "    if load_errors:\n",
    "        print(\"⚠️ Some candidates failed to load. See artifacts/vlm/load_failures.log\")\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "146db6a3-0ce2-4934-b121-72de00476298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered VLM candidates:\n",
      " - Qwen2-VL-2B-Instruct                      type=qwen2_vl      VL=True  score= 90\n",
      " - InternVL3_5-4B-Instruct                   type=internvl_chat  VL=True  score= 60\n"
     ]
    }
   ],
   "source": [
    "_ = discover_local_vlms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44fed26-2c44-483f-9d2a-3cc6b48f29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered VLM candidates:\n",
      " - Qwen2-VL-2B-Instruct                      type=qwen2_vl      VL=True  score= 90\n",
      " - InternVL3_5-4B-Instruct                   type=internvl_chat  VL=True  score= 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec97838b74a24af3996c88d2d4d50b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VLM answer saved: /home/manny-buff/projects/capstone/week11-hw/artifacts/vlm/answer_20251104_075010.json  (used: /home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct [Qwen2VL-native])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ok': True,\n",
       " 'used_model': '/home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct [Qwen2VL-native]',\n",
       " 'dtype': 'torch.bfloat16',\n",
       " 'cuda': '12.8',\n",
       " 'python': '3.11.9',\n",
       " 'query': 'faucet spout',\n",
       " 'images': ['/home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-012-033.png'],\n",
       " 'max_new_tokens': 128,\n",
       " 'answer': 'system\\nYou are a concise home-repair assistant. Use provided images to explain how to diagnose and reduce a faucet spout drip. Number steps (≤8); include tools & cautions.\\nuser\\nFrom the context and images, give clear steps to diagnose and fix a drip at the faucet spout. Assume a common compression or cartridge faucet.\\nassistant\\nTo diagnose and fix a drip at the faucet spout, follow these steps:\\n\\n### Tools Needed:\\n- Adjustable wrench\\n- Phillips head screwdriver\\n- Wrench for the faucet handle\\n- Adjustable wrench for the faucet handle\\n\\n### Caution:\\n- Turn off the water supply to the faucet before starting any work to avoid any accidents.\\n\\n### Step 1: Turn Off the Water Supply\\n- Locate the water supply valve for the faucet. It is usually located under the sink or on the wall near the faucet.\\n- Turn off the water supply to the faucet by closing the valve.\\n\\n### Step 2: Remove the Handle\\n-',\n",
       " 'wall_time_sec': 9.066,\n",
       " 'tried_models': ['/home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct [Qwen2VL-native]'],\n",
       " 'load_failures_logged': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Run once for the current query, K=2 (fallback to 1 handled by qwen_vl_answer) ----\n",
    "vlm_meta = qwen_vl_answer(QUERY, k_images=K, max_new=256)\n",
    "vlm_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fcc4a57-6407-41d5-842c-a860b25a44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VLM→SVD pipeline ready: call run_vlm_then_svd('faucet spout') or your query.\n"
     ]
    }
   ],
   "source": [
    "# Week11-HO-2 · Cell — Two-stage pipeline (VLM → SVD) with GPU exclusivity & light answer sanitization\n",
    "# Requires: qwen_vl_answer(...), svd_gpu_one_shot(...), select_images_for_query(...), gpu_gate helpers\n",
    "\n",
    "import os, json, re, time, platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "ART_PIPE = W11 / \"artifacts\" / \"pipeline\"\n",
    "ART_PIPE.mkdir(parents=True, exist_ok=True)\n",
    "METRICS_CSV = W11 / \"artifacts\" / \"metrics.csv\"\n",
    "\n",
    "# Light “sanity” cleaner to strip control chars & bizarre runs; keeps it readable for the rubric\n",
    "def _sanitize_text(s: str, max_len: int = 1200) -> str:\n",
    "    s = \"\".join(ch for ch in s if ch.isprintable())\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s*\\n\\s*\", \"\\n\", s)\n",
    "    s = re.sub(r\"([^\\w\\s.,;:!?()\\[\\]\\\"'-]{3,})\", \" \", s)  # drop long symbol runs\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n",
    "    if len(s) > max_len:\n",
    "        s = s[:max_len].rsplit(\"\\n\", 1)[0].rstrip() + \"\\n…\"\n",
    "    return s\n",
    "\n",
    "def run_vlm_then_svd(\n",
    "    query: str,\n",
    "    *,\n",
    "    k_images: int = 2,\n",
    "    vlm_tokens: int = 256,\n",
    "    svd_height: int = 576,\n",
    "    svd_width: int = 1024,\n",
    "    svd_frames: int = 24,\n",
    "    svd_fps: int = 8,\n",
    "    svd_steps: int = 18,\n",
    "    svd_motion: int = 127,\n",
    "    svd_noise: float = 0.02,\n",
    "    svd_prompt: str = (\"Short instructional clip showing the faucet spout area; subtle motion; \"\n",
    "                       \"neutral lighting; emphasize where drips occur and what to inspect.\")\n",
    "):\n",
    "    \"\"\"\n",
    "    One-click round trip: (1) vision-language answer, (2) image→video clip.\n",
    "    Returns a summary dict and writes artifacts to disk.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    # --- Stage A: VLM (uses its own GPU gate/backoffs)\n",
    "    vlm_meta = qwen_vl_answer(query, k_images=k_images, max_new=vlm_tokens)\n",
    "    if not vlm_meta.get(\"ok\"):\n",
    "        return {\"ok\": False, \"stage\": \"vlm\", \"error\": vlm_meta.get(\"error\", \"unknown\")}\n",
    "\n",
    "    used_images = vlm_meta.get(\"images\", []) or []\n",
    "    chosen_image = used_images[0] if used_images else None\n",
    "    vlm_answer = _sanitize_text(vlm_meta.get(\"answer\", \"\").strip())\n",
    "\n",
    "    # --- Stage B: SVD (uses its own GPU gate/backoffs)\n",
    "    svd_meta = None\n",
    "    if chosen_image:\n",
    "        svd_meta = svd_gpu_one_shot(\n",
    "            chosen_image,\n",
    "            prompt=svd_prompt,\n",
    "            height=svd_height,\n",
    "            width=svd_width,\n",
    "            frames=svd_frames,\n",
    "            fps=svd_fps,\n",
    "            steps=svd_steps,\n",
    "            motion=svd_motion,\n",
    "            noise=svd_noise,\n",
    "        )\n",
    "    else:\n",
    "        svd_meta = {\"ok\": False, \"error\": \"no_image_from_vlm\"}\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # --- Persist a small run report\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_json = ART_PIPE / f\"run_{ts}.json\"\n",
    "    payload = {\n",
    "        \"ok\": bool(vlm_meta.get(\"ok\") and svd_meta.get(\"ok\")),\n",
    "        \"query\": query,\n",
    "        \"started\": ts,\n",
    "        \"wall_time_sec\": round(t1 - t0, 3),\n",
    "        \"vlm\": {\n",
    "            \"ok\": vlm_meta.get(\"ok\"),\n",
    "            \"used_model\": vlm_meta.get(\"used_model\", vlm_meta.get(\"model\", \"\")),\n",
    "            \"images\": used_images,\n",
    "            \"answer_sanitized\": vlm_answer,\n",
    "            \"wall_time_sec\": vlm_meta.get(\"wall_time_sec\"),\n",
    "            \"artifacts\": [str(p) for p in (W11 / \"artifacts\" / \"vlm\").glob(\"answer_*.json\")][-1:]  # most recent\n",
    "        },\n",
    "        \"svd\": {\n",
    "            \"ok\": svd_meta.get(\"ok\"),\n",
    "            \"out_mp4\": svd_meta.get(\"out_mp4\", \"\"),\n",
    "            \"height\": svd_meta.get(\"height\"),\n",
    "            \"width\": svd_meta.get(\"width\"),\n",
    "            \"frames\": svd_meta.get(\"frames\"),\n",
    "            \"fps\": svd_meta.get(\"fps\"),\n",
    "            \"steps\": svd_meta.get(\"steps\"),\n",
    "            \"motion_bucket_id\": svd_meta.get(\"motion_bucket_id\"),\n",
    "            \"decode_chunk_size\": svd_meta.get(\"decode_chunk_size\"),\n",
    "            \"wall_time_sec\": svd_meta.get(\"wall_time_sec\"),\n",
    "        }\n",
    "    }\n",
    "    run_json.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # --- Append a combined “pipeline” metrics row (keeps your existing CSV header)\n",
    "    if not METRICS_CSV.exists():\n",
    "        METRICS_CSV.write_text(\n",
    "            \"timestamp,pipeline,domain,query,source_doc,source_page,image_path,model,frames,fps,steps,motion,noise,height,width,chunk,wall_s,video_s,RTF,out_mp4\\n\"\n",
    "        )\n",
    "    image_path_joined = \";\".join(used_images) if used_images else \"\"\n",
    "    with METRICS_CSV.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join([\n",
    "            datetime.now().isoformat(),\n",
    "            \"Pipeline(VLM→SVD)\",\n",
    "            \"home_repair\",\n",
    "            '\"' + query.replace('\"', \"'\") + '\"',\n",
    "            \"n/a\",\"-1\",\n",
    "            '\"' + image_path_joined + '\"',\n",
    "            '\"' + (vlm_meta.get(\"used_model\") or vlm_meta.get(\"model\",\"\")) + '\"',\n",
    "            str(svd_meta.get(\"frames\",\"\")),\n",
    "            str(svd_meta.get(\"fps\",\"\")),\n",
    "            str(svd_meta.get(\"steps\",\"\")),\n",
    "            str(svd_meta.get(\"motion_bucket_id\",\"\")),\n",
    "            str(svd_meta.get(\"noise_aug_strength\",\"\")),\n",
    "            str(svd_meta.get(\"height\",\"\")),\n",
    "            str(svd_meta.get(\"width\",\"\")),\n",
    "            str(svd_meta.get(\"decode_chunk_size\",\"\")),\n",
    "            str(round((vlm_meta.get(\"wall_time_sec\") or 0) + (svd_meta.get(\"wall_time_sec\") or 0), 3)),\n",
    "            str(svd_meta.get(\"video_duration_sec\",\"\")),\n",
    "            str(svd_meta.get(\"RTF\",\"\")),\n",
    "            '\"' + svd_meta.get(\"out_mp4\",\"\") + '\"',\n",
    "        ]) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Pipeline complete. Run report: {run_json}\")\n",
    "    if not payload[\"ok\"]:\n",
    "        print(\"⚠️ One stage failed. See payload and per-stage artifacts for details.\")\n",
    "    return payload\n",
    "\n",
    "print(\"✅ VLM→SVD pipeline ready: call run_vlm_then_svd('faucet spout') or your query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c819abf9-20a5-499e-a63b-48e548264b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered VLM candidates:\n",
      " - Qwen2-VL-2B-Instruct                      type=qwen2_vl      VL=True  score= 90\n",
      " - InternVL3_5-4B-Instruct                   type=internvl_chat  VL=True  score= 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69cbeb9bc1b419786455bc73683da1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VLM answer saved: /home/manny-buff/projects/capstone/week11-hw/artifacts/vlm/answer_20251104_085901.json  (used: /home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct [Qwen2VL-native])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2d216ef77a4d50b0bbc098f4c2cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251104_090553.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6156b0985c5495e8c9b8cf5384e1b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=bf16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251104_091247.mp4\n",
      "    used: {'dtype': 'torch.bfloat16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab853c8221e64b49949302433ec8e99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp16, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251104_091954.mp4\n",
      "    used: {'dtype': 'torch.float16', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 24, 'chunk': 6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9e867f01c84c7fa725425cec74ba53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=24, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=20, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=4 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=2 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=16, chunk=1 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=8 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=6 -> reducing...\n",
      "[OOM fallback] dtype=fp32, offload=sequential, 576x1024, frames=12, chunk=4 -> reducing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SVD (memsafe) done: /home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251104_093051.mp4\n",
      "    used: {'dtype': 'torch.float32', 'offload': 'sequential', 'H': 576, 'W': 1024, 'frames': 12, 'chunk': 2}\n",
      "✅ Pipeline complete. Run report: /home/manny-buff/projects/capstone/week11-hw/artifacts/pipeline/run_20251104_093052.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ok': True,\n",
       " 'query': 'faucet spout',\n",
       " 'started': '20251104_093052',\n",
       " 'wall_time_sec': 1918.207,\n",
       " 'vlm': {'ok': True,\n",
       "  'used_model': '/home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct [Qwen2VL-native]',\n",
       "  'images': ['/home/manny-buff/projects/capstone/week11-hw/artifacts/graph_v2/images/1001 do-it-yourself hints & tips  tricks/img-012-033.png'],\n",
       "  'answer_sanitized': 'systemYou are a concise home-repair assistant. Use provided images to explain how to diagnose and reduce a faucet spout drip. Number steps (≤8); include tools & cautions.userFrom the context and images, give clear steps to diagnose and fix a drip at the faucet spout. Assume a common compression or cartridge faucet.assistantTo diagnose and fix a drip at the faucet spout, follow these steps:  Tools Needed:- Adjustable wrench- Phillips head screwdriver- Wrench for the faucet handle- Adjustable wrench for the faucet handle  Caution:- Turn off the water supply to the faucet before starting any work to avoid any accidents.  Step 1: Turn Off the Water Supply- Locate the water supply valve for the faucet. It is usually located under the sink or on the wall near the faucet.- Turn off the water supply to the faucet by closing the valve.  Step 2: Remove the Handle-',\n",
       "  'wall_time_sec': 6.818,\n",
       "  'artifacts': ['/home/manny-buff/projects/capstone/week11-hw/artifacts/vlm/answer_20251104_075010.json']},\n",
       " 'svd': {'ok': True,\n",
       "  'out_mp4': '/home/manny-buff/projects/capstone/week11-hw/visual_outputs/svd_i2v_mem_20251104_093051.mp4',\n",
       "  'height': 576,\n",
       "  'width': 1024,\n",
       "  'frames': 12,\n",
       "  'fps': 8,\n",
       "  'steps': 18,\n",
       "  'motion_bucket_id': 127,\n",
       "  'decode_chunk_size': 2,\n",
       "  'wall_time_sec': 161.774}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_summary = run_vlm_then_svd(\"faucet spout\")\n",
    "run_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42017a43-114c-41e9-940c-cba8d63b0282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Report updated: /home/manny-buff/projects/capstone/week11-hw/reports/Report.md\n",
      "✅ Manifest written: /home/manny-buff/projects/capstone/week11-hw/artifacts/curated/MANIFEST.txt\n",
      "Curated copies:\n",
      " - Image: /home/manny-buff/projects/capstone/week11-hw/artifacts/curated/faucet_spout.png\n",
      " - Video: /home/manny-buff/projects/capstone/week11-hw/artifacts/curated/svd_example.mp4\n"
     ]
    }
   ],
   "source": [
    "# Week11-HO-2 · Finalize & curate artifacts + append Report section\n",
    "from pathlib import Path\n",
    "import shutil, time, json\n",
    "\n",
    "W11 = Path(\"/home/manny-buff/projects/capstone/week11-hw\")\n",
    "REPORT = W11 / \"reports\" / \"Report.md\"\n",
    "CURATED = W11 / \"artifacts\" / \"curated\"\n",
    "CURATED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- (A) Select artifacts ---\n",
    "# Image: hardcoded faucet path you provided\n",
    "src_img = W11 / \"artifacts\" / \"graph_v2\" / \"images\" / \"Safe & Sound _ A Renter-Friendly Guide to Home Repair\" / \"img-112-085.png\"\n",
    "\n",
    "# Video: prefer the specific one; else newest svd_i2v_mem*.mp4 under visual_outputs\n",
    "VO = W11 / \"visual_outputs\"\n",
    "prefer_name = \"svd_i2v_mem_20251103_180307.mp4\"\n",
    "cand_video = VO / prefer_name\n",
    "if not cand_video.exists():\n",
    "    svds = sorted(VO.glob(\"svd_i2v_mem*.mp4\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    cand_video = svds[0] if svds else None\n",
    "\n",
    "# --- (B) Copy into curated/ with stable names ---\n",
    "cur_img = CURATED / \"faucet_spout.png\"\n",
    "cur_vid = CURATED / \"svd_example.mp4\"\n",
    "\n",
    "copied = {}\n",
    "missing = []\n",
    "\n",
    "def _copy(src: Path, dst: Path):\n",
    "    if not src or not src.exists():\n",
    "        return False\n",
    "    try:\n",
    "        shutil.copy2(src, dst)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if _copy(src_img, cur_img):\n",
    "    copied[\"image\"] = str(cur_img)\n",
    "else:\n",
    "    missing.append(str(src_img))\n",
    "\n",
    "if cand_video and _copy(cand_video, cur_vid):\n",
    "    copied[\"video\"] = str(cur_vid)\n",
    "else:\n",
    "    missing.append(str(cand_video) if cand_video else \"NO_VIDEO_FOUND\")\n",
    "\n",
    "# --- (C) Append to Report.md (GitHub supports <u> for underline) ---\n",
    "ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "img_rel = Path(\"../artifacts/curated/faucet_spout.png\") if \"image\" in copied else None\n",
    "vid_rel = Path(\"../artifacts/curated/svd_example.mp4\") if \"video\" in copied else None\n",
    "\n",
    "section = []\n",
    "section.append(\"\\n---\\n\")\n",
    "section.append(\"## Week11-HO-2 Summary (VLM → SVD)\\n\")\n",
    "section.append(f\"_Append timestamp: {ts}_\\n\\n\")\n",
    "section.append(\"**What worked**\\n\")\n",
    "section.append(\"- Vision-Language (Qwen2-VL) produced concise, step-by-step faucet guidance.\\n\")\n",
    "section.append(\"- Stable Video Diffusion produced short clips when a suitable source image was selected.\\n\\n\")\n",
    "section.append(\"**What didn’t**\\n\")\n",
    "section.append(\"- Automated image retrieval frequently surfaced unusable pages (blank/over-compressed/repetitive).\\n\")\n",
    "section.append(\"- Video fidelity was highly sensitive to source image quality and parameters.\\n\\n\")\n",
    "section.append(\"**Conclusion**\\n\")\n",
    "section.append(\"- The end-to-end process <u>could</u> be accomplished, but a **customized image selector** is required and will still need **significant human intervention** to curate usable inputs. \")\n",
    "section.append(\"Video generation is even more sensitive and demands careful, manual tuning.\\n\\n\")\n",
    "if img_rel:\n",
    "    section.append(f\"**Curated image (for rubric/demo):** `artifacts/curated/faucet_spout.png`\\n\\n\")\n",
    "    section.append(f\"![Faucet spout diagram]({img_rel})\\n\\n\")\n",
    "if vid_rel:\n",
    "    section.append(f\"**Curated video (for rubric/demo):** `artifacts/curated/svd_example.mp4`\\n\\n\")\n",
    "    section.append(f\"[Download curated video]({vid_rel})\\n\\n\")\n",
    "\n",
    "REPORT.parent.mkdir(parents=True, exist_ok=True)\n",
    "with REPORT.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(section))\n",
    "\n",
    "# --- (D) Write a tiny manifest for Git staging (only curated items + Report) ---\n",
    "manifest = {\n",
    "    \"include\": [\n",
    "        \"reports/Report.md\",\n",
    "        \"artifacts/curated/faucet_spout.png\" if \"image\" in copied else None,\n",
    "        \"artifacts/curated/svd_example.mp4\" if \"video\" in copied else None,\n",
    "    ]\n",
    "}\n",
    "manifest[\"include\"] = [p for p in manifest[\"include\"] if p]  # drop Nones\n",
    "(CURATED / \"MANIFEST.txt\").write_text(\"\\n\".join(manifest[\"include\"]) + \"\\n\", encoding=\"utf-8\")\n",
    "(CURATED / \"MANIFEST.json\").write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- (E) Print status ---\n",
    "print(\"✅ Report updated:\", REPORT)\n",
    "print(\"✅ Manifest written:\", CURATED / \"MANIFEST.txt\")\n",
    "print(\"Curated copies:\")\n",
    "print(\" - Image:\", copied.get(\"image\", \"MISSING\"))\n",
    "print(\" - Video:\", copied.get(\"video\", \"MISSING\"))\n",
    "if missing:\n",
    "    print(\"⚠️ Missing source(s):\")\n",
    "    for m in missing:\n",
    "        print(\"   -\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac16a2b-7d99-4059-8d36-be84df0f185d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (core-rag)",
   "language": "python",
   "name": "core-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
